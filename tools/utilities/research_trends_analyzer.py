#!/usr/bin/env python3
"""
ç ”ç©¶é¢†åŸŸå‘å±•è¶‹åŠ¿æ·±åº¦åˆ†æå™¨
åŒ…æ‹¬æ€»ä½“è¶‹åŠ¿å’Œåˆ†ä¼šè®®è¶‹åŠ¿åˆ†æ
"""

import json
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Tuple
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class ResearchTrendsAnalyzer:
    """ç ”ç©¶é¢†åŸŸå‘å±•è¶‹åŠ¿æ·±åº¦åˆ†æå™¨"""
    
    def __init__(self, data_path: str = "outputs/analysis/comprehensive_analysis.json"):
        self.data_path = Path(data_path)
        self.data = self.load_data()
        self.output_dir = Path("outputs/research_trends")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # å¹´ä»½èŒƒå›´
        self.years = list(range(2018, 2025))
        self.analysis_years = list(range(2018, 2024))  # æ’é™¤2024å¹´ï¼ˆæ•°æ®ä¸å®Œæ•´ï¼‰
        
    def load_data(self) -> Dict:
        """åŠ è½½åˆ†ææ•°æ®"""
        with open(self.data_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def analyze_overall_trends(self) -> Dict[str, Any]:
        """åˆ†ææ€»ä½“ç ”ç©¶é¢†åŸŸå‘å±•è¶‹åŠ¿"""
        print("ğŸ“Š åˆ†ææ€»ä½“ç ”ç©¶é¢†åŸŸå‘å±•è¶‹åŠ¿...")
        
        field_trends = self.data['field_analysis']['field_trends']
        
        overall_analysis = {
            'trend_patterns': {},
            'growth_analysis': {},
            'dominance_shifts': {},
            'emerging_fields': {},
            'declining_fields': {},
            'stability_analysis': {}
        }
        
        for field, yearly_data in field_trends.items():
            values = [yearly_data.get(str(year), 0) for year in self.analysis_years]
            
            # 1. è¶‹åŠ¿æ¨¡å¼åˆ†æ
            trend_pattern = self.analyze_trend_pattern(values, field)
            overall_analysis['trend_patterns'][field] = trend_pattern
            
            # 2. å¢é•¿åˆ†æ
            growth_analysis = self.analyze_growth_metrics(values, field)
            overall_analysis['growth_analysis'][field] = growth_analysis
            
            # 3. ç¨³å®šæ€§åˆ†æ
            stability = self.analyze_stability(values, field)
            overall_analysis['stability_analysis'][field] = stability
        
        # 4. é¢†åŸŸä¸»å¯¼åœ°ä½å˜åŒ–
        overall_analysis['dominance_shifts'] = self.analyze_dominance_shifts(field_trends)
        
        # 5. æ–°å…´å’Œè¡°é€€é¢†åŸŸè¯†åˆ«
        overall_analysis['emerging_fields'] = self.identify_emerging_fields(field_trends)
        overall_analysis['declining_fields'] = self.identify_declining_fields(field_trends)
        
        return overall_analysis
    
    def analyze_trend_pattern(self, values: List[int], field: str) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªé¢†åŸŸçš„è¶‹åŠ¿æ¨¡å¼"""
        # çº¿æ€§è¶‹åŠ¿
        x = np.array(range(len(values)))
        linear_slope, linear_intercept, r_value, p_value, std_err = stats.linregress(x, values)
        
        # å¤šé¡¹å¼è¶‹åŠ¿ï¼ˆäºŒæ¬¡ï¼‰
        poly_features = PolynomialFeatures(degree=2)
        x_poly = poly_features.fit_transform(x.reshape(-1, 1))
        poly_model = LinearRegression()
        poly_model.fit(x_poly, values)
        poly_score = poly_model.score(x_poly, values)
        
        # è¶‹åŠ¿ç±»å‹åˆ¤æ–­
        if abs(linear_slope) < 10:
            trend_type = "ç¨³å®šå‹"
        elif linear_slope > 50:
            trend_type = "å¼ºå¢é•¿å‹"
        elif linear_slope > 20:
            trend_type = "å¢é•¿å‹"
        elif linear_slope < -20:
            trend_type = "è¡°é€€å‹"
        else:
            trend_type = "æ³¢åŠ¨å‹"
        
        # å‘¨æœŸæ€§æ£€æµ‹
        is_cyclical = self.detect_cyclical_pattern(values)
        
        return {
            'linear_slope': round(linear_slope, 2),
            'r_squared': round(r_value**2, 3),
            'p_value': round(p_value, 4),
            'trend_type': trend_type,
            'polynomial_fit': round(poly_score, 3),
            'is_cyclical': is_cyclical,
            'trend_strength': abs(linear_slope),
            'trend_significance': 'significant' if p_value < 0.05 else 'not_significant'
        }
    
    def detect_cyclical_pattern(self, values: List[int]) -> bool:
        """æ£€æµ‹æ˜¯å¦å­˜åœ¨å‘¨æœŸæ€§æ¨¡å¼"""
        if len(values) < 4:
            return False
        
        # ç®€å•çš„å³°è°·æ£€æµ‹
        peaks = 0
        valleys = 0
        
        for i in range(1, len(values) - 1):
            if values[i] > values[i-1] and values[i] > values[i+1]:
                peaks += 1
            elif values[i] < values[i-1] and values[i] < values[i+1]:
                valleys += 1
        
        return peaks >= 2 or valleys >= 2
    
    def analyze_growth_metrics(self, values: List[int], field: str) -> Dict[str, Any]:
        """åˆ†æå¢é•¿æŒ‡æ ‡"""
        if len(values) < 2:
            return {}
        
        # æ€»å¢é•¿ç‡
        total_growth = ((values[-1] - values[0]) / values[0] * 100) if values[0] > 0 else 0
        
        # å¹´å‡å¤åˆå¢é•¿ç‡ (CAGR)
        years = len(values) - 1
        cagr = (pow(values[-1] / values[0], 1/years) - 1) * 100 if values[0] > 0 and values[-1] > 0 else 0
        
        # å¢é•¿åŠ é€Ÿåº¦ï¼ˆäºŒé˜¶å¯¼æ•°ï¼‰
        growth_rates = [(values[i+1] - values[i])/values[i]*100 if values[i] > 0 else 0 
                       for i in range(len(values)-1)]
        acceleration = np.diff(growth_rates).mean() if len(growth_rates) > 1 else 0
        
        # å¢é•¿ä¸€è‡´æ€§ï¼ˆå˜å¼‚ç³»æ•°ï¼‰
        growth_consistency = 1 - (np.std(growth_rates) / np.mean(growth_rates)) if np.mean(growth_rates) != 0 else 0
        
        # å¢é•¿é˜¶æ®µè¯†åˆ«
        growth_phase = self.identify_growth_phase(values, growth_rates)
        
        return {
            'total_growth_rate': round(total_growth, 1),
            'cagr': round(cagr, 1),
            'growth_acceleration': round(acceleration, 2),
            'growth_consistency': max(0, round(growth_consistency, 3)),
            'growth_phase': growth_phase,
            'peak_year_index': values.index(max(values)),
            'valley_year_index': values.index(min(values))
        }
    
    def identify_growth_phase(self, values: List[int], growth_rates: List[float]) -> str:
        """è¯†åˆ«å¢é•¿é˜¶æ®µ"""
        recent_growth = np.mean(growth_rates[-2:]) if len(growth_rates) >= 2 else growth_rates[-1] if growth_rates else 0
        early_growth = np.mean(growth_rates[:2]) if len(growth_rates) >= 2 else growth_rates[0] if growth_rates else 0
        
        if recent_growth > 20:
            return "å¿«é€Ÿå¢é•¿æœŸ"
        elif recent_growth > 10:
            return "ç¨³å®šå¢é•¿æœŸ"
        elif recent_growth > -10:
            return "æˆç†Ÿç¨³å®šæœŸ"
        elif recent_growth > -20:
            return "ç¼“æ…¢è¡°é€€æœŸ"
        else:
            return "å¿«é€Ÿè¡°é€€æœŸ"
    
    def analyze_stability(self, values: List[int], field: str) -> Dict[str, Any]:
        """åˆ†æç¨³å®šæ€§æŒ‡æ ‡"""
        # å˜å¼‚ç³»æ•°
        cv = np.std(values) / np.mean(values) if np.mean(values) > 0 else float('inf')
        
        # ç¨³å®šæ€§è¯„åˆ† (0-1ï¼Œ1ä¸ºæœ€ç¨³å®š)
        stability_score = max(0, 1 - cv)
        
        # æ³¢åŠ¨æ€§åˆ†æ
        if cv < 0.2:
            volatility_level = "ä½æ³¢åŠ¨"
        elif cv < 0.5:
            volatility_level = "ä¸­ç­‰æ³¢åŠ¨"
        else:
            volatility_level = "é«˜æ³¢åŠ¨"
        
        return {
            'coefficient_variation': round(cv, 3),
            'stability_score': round(stability_score, 3),
            'volatility_level': volatility_level,
            'max_year_variation': max(values) - min(values)
        }
    
    def analyze_dominance_shifts(self, field_trends: Dict) -> Dict[str, Any]:
        """åˆ†æé¢†åŸŸä¸»å¯¼åœ°ä½å˜åŒ–"""
        dominance_analysis = {}
        
        for year in self.analysis_years:
            year_str = str(year)
            year_totals = {}
            
            for field, yearly_data in field_trends.items():
                year_totals[field] = yearly_data.get(year_str, 0)
            
            # è®¡ç®—å¸‚åœºä»½é¢
            total_papers = sum(year_totals.values())
            if total_papers > 0:
                market_shares = {field: count/total_papers*100 
                               for field, count in year_totals.items()}
                dominance_analysis[year] = {
                    'top_field': max(market_shares, key=market_shares.get),
                    'top_share': max(market_shares.values()),
                    'market_shares': market_shares,
                    'concentration_index': self.calculate_concentration_index(market_shares)
                }
        
        # åˆ†æä¸»å¯¼åœ°ä½å˜åŒ–
        dominance_shifts = []
        for i in range(1, len(self.analysis_years)):
            prev_year = self.analysis_years[i-1]
            curr_year = self.analysis_years[i]
            
            if prev_year in dominance_analysis and curr_year in dominance_analysis:
                prev_leader = dominance_analysis[prev_year]['top_field']
                curr_leader = dominance_analysis[curr_year]['top_field']
                
                if prev_leader != curr_leader:
                    dominance_shifts.append({
                        'year': curr_year,
                        'from': prev_leader,
                        'to': curr_leader,
                        'prev_share': dominance_analysis[prev_year]['top_share'],
                        'curr_share': dominance_analysis[curr_year]['top_share']
                    })
        
        return {
            'yearly_dominance': dominance_analysis,
            'leadership_changes': dominance_shifts,
            'most_dominant_overall': self.find_most_dominant_field(field_trends)
        }
    
    def calculate_concentration_index(self, market_shares: Dict[str, float]) -> float:
        """è®¡ç®—å¸‚åœºé›†ä¸­åº¦æŒ‡æ•° (HHI)"""
        return sum(share**2 for share in market_shares.values()) / 100
    
    def find_most_dominant_field(self, field_trends: Dict) -> Dict[str, Any]:
        """æ‰¾å‡ºæ€»ä½“æœ€ä¸»å¯¼çš„é¢†åŸŸ"""
        total_papers = {}
        avg_shares = {}
        
        for field, yearly_data in field_trends.items():
            total_papers[field] = sum(yearly_data.get(str(year), 0) for year in self.analysis_years)
            
            # è®¡ç®—å¹³å‡å¸‚åœºä»½é¢
            yearly_totals = [sum(yearly_data.get(str(year), 0) for yearly_data in field_trends.values()) 
                           for year in self.analysis_years]
            field_yearly = [yearly_data.get(str(year), 0) for year in self.analysis_years]
            avg_shares[field] = np.mean([field_yearly[i]/yearly_totals[i]*100 if yearly_totals[i] > 0 else 0 
                                       for i in range(len(field_yearly))])
        
        most_papers = max(total_papers, key=total_papers.get)
        highest_avg_share = max(avg_shares, key=avg_shares.get)
        
        return {
            'by_total_papers': {
                'field': most_papers,
                'papers': total_papers[most_papers]
            },
            'by_average_share': {
                'field': highest_avg_share,
                'avg_share': round(avg_shares[highest_avg_share], 1)
            }
        }
    
    def identify_emerging_fields(self, field_trends: Dict) -> Dict[str, Any]:
        """è¯†åˆ«æ–°å…´é¢†åŸŸ"""
        emerging_criteria = {}
        
        for field, yearly_data in field_trends.items():
            values = [yearly_data.get(str(year), 0) for year in self.analysis_years]
            
            # æ–°å…´é¢†åŸŸæ ‡å‡†ï¼š
            # 1. æ—©æœŸè§„æ¨¡å°
            # 2. è¿‘æœŸå¢é•¿å¿«
            # 3. å¢é•¿åŠ é€Ÿåº¦é«˜
            
            early_avg = np.mean(values[:2]) if len(values) >= 2 else values[0]
            recent_avg = np.mean(values[-2:]) if len(values) >= 2 else values[-1]
            
            if early_avg > 0:
                growth_multiple = recent_avg / early_avg
                growth_rate = (growth_multiple - 1) * 100
            else:
                growth_multiple = float('inf') if recent_avg > 0 else 1
                growth_rate = float('inf') if recent_avg > 0 else 0
            
            # è®¡ç®—å¢é•¿åŠ é€Ÿåº¦
            growth_rates = [(values[i+1] - values[i])/values[i]*100 if values[i] > 0 else 0 
                           for i in range(len(values)-1)]
            acceleration = np.diff(growth_rates).mean() if len(growth_rates) > 1 else 0
            
            emerging_score = 0
            if early_avg < 100:  # æ—©æœŸè§„æ¨¡å°
                emerging_score += 1
            if growth_rate > 100:  # é«˜å¢é•¿
                emerging_score += 2
            if acceleration > 0:  # æ­£åŠ é€Ÿåº¦
                emerging_score += 1
            
            emerging_criteria[field] = {
                'early_average': early_avg,
                'recent_average': recent_avg,
                'growth_multiple': round(growth_multiple, 1) if growth_multiple != float('inf') else 'inf',
                'growth_rate': round(growth_rate, 1) if growth_rate != float('inf') else 'inf',
                'acceleration': round(acceleration, 2),
                'emerging_score': emerging_score,
                'is_emerging': emerging_score >= 3
            }
        
        # æŒ‰æ–°å…´è¯„åˆ†æ’åº
        emerging_fields = {k: v for k, v in emerging_criteria.items() if v['is_emerging']}
        emerging_sorted = dict(sorted(emerging_fields.items(), 
                                    key=lambda x: x[1]['emerging_score'], reverse=True))
        
        return {
            'emerging_fields': emerging_sorted,
            'emerging_count': len(emerging_fields),
            'all_scores': emerging_criteria
        }
    
    def identify_declining_fields(self, field_trends: Dict) -> Dict[str, Any]:
        """è¯†åˆ«è¡°é€€é¢†åŸŸ"""
        declining_criteria = {}
        
        for field, yearly_data in field_trends.items():
            values = [yearly_data.get(str(year), 0) for year in self.analysis_years]
            
            # è¡°é€€é¢†åŸŸæ ‡å‡†ï¼š
            # 1. è´Ÿå¢é•¿è¶‹åŠ¿
            # 2. å¸‚åœºä»½é¢ä¸‹é™
            # 3. è¿ç»­ä¸‹é™æœŸ
            
            # çº¿æ€§è¶‹åŠ¿
            x = np.array(range(len(values)))
            slope, _, r_value, p_value, _ = stats.linregress(x, values)
            
            # è¿ç»­ä¸‹é™æœŸæ£€æµ‹
            consecutive_declines = 0
            max_consecutive_declines = 0
            for i in range(1, len(values)):
                if values[i] < values[i-1]:
                    consecutive_declines += 1
                    max_consecutive_declines = max(max_consecutive_declines, consecutive_declines)
                else:
                    consecutive_declines = 0
            
            # è¡°é€€è¯„åˆ†
            declining_score = 0
            if slope < -10:  # è´Ÿè¶‹åŠ¿
                declining_score += 2
            if max_consecutive_declines >= 2:  # è¿ç»­ä¸‹é™
                declining_score += 1
            if r_value**2 > 0.5 and slope < 0:  # æ˜¾è‘—è´Ÿè¶‹åŠ¿
                declining_score += 1
            
            declining_criteria[field] = {
                'slope': round(slope, 2),
                'r_squared': round(r_value**2, 3),
                'max_consecutive_declines': max_consecutive_declines,
                'declining_score': declining_score,
                'is_declining': declining_score >= 2
            }
        
        declining_fields = {k: v for k, v in declining_criteria.items() if v['is_declining']}
        declining_sorted = dict(sorted(declining_fields.items(), 
                                     key=lambda x: x[1]['declining_score'], reverse=True))
        
        return {
            'declining_fields': declining_sorted,
            'declining_count': len(declining_fields),
            'all_scores': declining_criteria
        }
    
    def analyze_conference_trends(self) -> Dict[str, Any]:
        """åˆ†æå„ä¼šè®®çš„ç ”ç©¶é¢†åŸŸè¶‹åŠ¿"""
        print("ğŸ›ï¸ åˆ†æå„ä¼šè®®ç ”ç©¶é¢†åŸŸè¶‹åŠ¿...")
        
        conference_specialization = self.data['cross_analysis']['conference_scenario_specialization']
        conference_yearly = self.data['temporal_analysis']['conference_yearly_distribution']
        
        conference_analysis = {}
        
        # åˆ†ææ¯ä¸ªä¼šè®®çš„é¢†åŸŸåå¥½
        for conference in ['NeuRIPS', 'ICLR', 'AAAI']:
            conference_analysis[conference] = self.analyze_single_conference(
                conference, conference_specialization, conference_yearly
            )
        
        # ä¼šè®®é—´æ¯”è¾ƒåˆ†æ
        conference_analysis['comparative_analysis'] = self.compare_conferences(conference_analysis)
        
        return conference_analysis
    
    def analyze_single_conference(self, conference: str, specialization: Dict, yearly_dist: Dict) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªä¼šè®®çš„ç ”ç©¶é¢†åŸŸè¶‹åŠ¿"""
        
        # è¯¥ä¼šè®®åœ¨å„é¢†åŸŸçš„ä¸“ä¸šåŒ–ç¨‹åº¦
        conference_specialization = {}
        for field, conf_data in specialization.items():
            if conference in conf_data:
                conference_specialization[field] = conf_data[conference]
        
        # æŒ‰ä¸“ä¸šåŒ–ç¨‹åº¦æ’åº
        sorted_specialization = dict(sorted(conference_specialization.items(), 
                                          key=lambda x: x[1], reverse=True))
        
        # è®¡ç®—è¯¥ä¼šè®®çš„å¹´åº¦å¢é•¿è¶‹åŠ¿
        yearly_papers = []
        for year in self.analysis_years:
            yearly_papers.append(yearly_dist.get(str(year), {}).get(conference, 0))
        
        # å¢é•¿åˆ†æ
        growth_analysis = self.analyze_growth_metrics(yearly_papers, conference)
        
        # é¢†åŸŸé›†ä¸­åº¦åˆ†æ
        concentration = self.calculate_field_concentration(conference_specialization)
        
        # è¯†åˆ«è¯¥ä¼šè®®çš„æ ¸å¿ƒé¢†åŸŸå’Œæ–°å…´é¢†åŸŸ
        core_fields = [field for field, spec in sorted_specialization.items() if spec > 0.1][:5]
        emerging_in_conference = [field for field, spec in sorted_specialization.items() 
                                if 0.05 < spec < 0.1]
        
        return {
            'specialization_scores': sorted_specialization,
            'core_fields': core_fields,
            'emerging_fields': emerging_in_conference,
            'field_concentration': concentration,
            'yearly_papers': yearly_papers,
            'growth_analysis': growth_analysis,
            'total_papers': sum(yearly_papers),
            'peak_year': self.analysis_years[yearly_papers.index(max(yearly_papers))] if yearly_papers else None
        }
    
    def calculate_field_concentration(self, specialization: Dict[str, float]) -> Dict[str, Any]:
        """è®¡ç®—é¢†åŸŸé›†ä¸­åº¦"""
        values = list(specialization.values())
        
        # Herfindahl-Hirschman Index
        hhi = sum(v**2 for v in values) * 10000
        
        # å‰3é¢†åŸŸé›†ä¸­åº¦
        top3_concentration = sum(sorted(values, reverse=True)[:3])
        
        # åŸºå°¼ç³»æ•°
        gini = self.calculate_gini_coefficient(values)
        
        return {
            'hhi': round(hhi, 1),
            'top3_concentration': round(top3_concentration, 3),
            'gini_coefficient': round(gini, 3),
            'concentration_level': self.classify_concentration(hhi)
        }
    
    def calculate_gini_coefficient(self, values: List[float]) -> float:
        """è®¡ç®—åŸºå°¼ç³»æ•°"""
        if len(values) == 0:
            return 0
        
        sorted_values = sorted(values)
        n = len(values)
        cumsum = np.cumsum(sorted_values)
        
        return (n + 1 - 2 * sum((n + 1 - i) * y for i, y in enumerate(sorted_values, 1))) / (n * sum(sorted_values))
    
    def classify_concentration(self, hhi: float) -> str:
        """åˆ†ç±»é›†ä¸­åº¦æ°´å¹³"""
        if hhi < 1500:
            return "ä½é›†ä¸­åº¦"
        elif hhi < 2500:
            return "ä¸­ç­‰é›†ä¸­åº¦"
        else:
            return "é«˜é›†ä¸­åº¦"
    
    def compare_conferences(self, conference_analysis: Dict) -> Dict[str, Any]:
        """æ¯”è¾ƒå„ä¼šè®®é—´çš„å·®å¼‚"""
        conferences = ['NeuRIPS', 'ICLR', 'AAAI']
        
        comparison = {
            'diversity_ranking': {},
            'growth_ranking': {},
            'specialization_uniqueness': {},
            'conference_evolution': {}
        }
        
        # å¤šæ ·æ€§æ’åï¼ˆåŸºäºåŸºå°¼ç³»æ•°ï¼Œè¶Šå°è¶Šå¤šæ ·ï¼‰
        diversity_scores = {}
        for conf in conferences:
            if conf in conference_analysis:
                gini = conference_analysis[conf]['field_concentration']['gini_coefficient']
                diversity_scores[conf] = gini
        
        comparison['diversity_ranking'] = dict(sorted(diversity_scores.items(), key=lambda x: x[1]))
        
        # å¢é•¿é€Ÿåº¦æ’å
        growth_scores = {}
        for conf in conferences:
            if conf in conference_analysis:
                cagr = conference_analysis[conf]['growth_analysis'].get('cagr', 0)
                growth_scores[conf] = cagr
        
        comparison['growth_ranking'] = dict(sorted(growth_scores.items(), key=lambda x: x[1], reverse=True))
        
        # ä¸“ä¸šåŒ–ç‹¬ç‰¹æ€§åˆ†æ
        for conf in conferences:
            if conf in conference_analysis:
                unique_fields = []
                conf_spec = conference_analysis[conf]['specialization_scores']
                
                for field, score in conf_spec.items():
                    # æ£€æŸ¥è¯¥é¢†åŸŸæ˜¯å¦åœ¨æ­¤ä¼šè®®ä¸­ç‰¹åˆ«çªå‡º
                    is_unique = True
                    for other_conf in conferences:
                        if other_conf != conf and other_conf in conference_analysis:
                            other_score = conference_analysis[other_conf]['specialization_scores'].get(field, 0)
                            if other_score >= score * 0.8:  # å¦‚æœå…¶ä»–ä¼šè®®ä¹Ÿå¾ˆå¼ºï¼Œåˆ™ä¸ç®—ç‹¬ç‰¹
                                is_unique = False
                                break
                    
                    if is_unique and score > 0.05:  # æœ€å°é˜ˆå€¼
                        unique_fields.append(field)
                
                comparison['specialization_uniqueness'][conf] = unique_fields
        
        return comparison
    
    def generate_predictions(self, overall_analysis: Dict, conference_analysis: Dict) -> Dict[str, Any]:
        """ç”Ÿæˆå‘å±•è¶‹åŠ¿é¢„æµ‹"""
        print("ğŸ”® ç”Ÿæˆç ”ç©¶é¢†åŸŸå‘å±•é¢„æµ‹...")
        
        predictions = {
            '2025_forecasts': {},
            'hot_topics_prediction': {},
            'conference_evolution_prediction': {},
            'risk_assessment': {}
        }
        
        # åŸºäºå†å²è¶‹åŠ¿é¢„æµ‹2025å¹´å„é¢†åŸŸè®ºæ–‡æ•°é‡
        field_trends = self.data['field_analysis']['field_trends']
        
        for field, yearly_data in field_trends.items():
            values = [yearly_data.get(str(year), 0) for year in self.analysis_years]
            
            # ä½¿ç”¨çº¿æ€§å›å½’é¢„æµ‹
            x = np.array(range(len(values))).reshape(-1, 1)
            model = LinearRegression()
            model.fit(x, values)
            
            # é¢„æµ‹2025å¹´ (ç´¢å¼•ä¸º6)
            predicted_2025 = max(0, int(model.predict([[len(values)]])[0]))
            
            # ç½®ä¿¡åŒºé—´ä¼°ç®—
            trend_data = overall_analysis['trend_patterns'][field]
            confidence = 'high' if trend_data['r_squared'] > 0.7 else 'medium' if trend_data['r_squared'] > 0.4 else 'low'
            
            predictions['2025_forecasts'][field] = {
                'predicted_papers': predicted_2025,
                'confidence_level': confidence,
                'trend_basis': trend_data['trend_type']
            }
        
        # é¢„æµ‹çƒ­ç‚¹è¯é¢˜
        emerging_fields = overall_analysis['emerging_fields']['emerging_fields']
        growth_analysis = overall_analysis['growth_analysis']
        
        hot_topics = []
        for field in emerging_fields.keys():
            if field in growth_analysis:
                cagr = growth_analysis[field]['cagr']
                if cagr > 30:  # é«˜å¢é•¿ç‡
                    hot_topics.append({
                        'field': field,
                        'reason': f'CAGR {cagr}%, æ–°å…´é¢†åŸŸ',
                        'priority': 'high'
                    })
        
        # æ·»åŠ ç¨³å®šå¢é•¿çš„å¤§é¢†åŸŸ
        for field, analysis in growth_analysis.items():
            if analysis['cagr'] > 15 and analysis['growth_phase'] in ['ç¨³å®šå¢é•¿æœŸ', 'å¿«é€Ÿå¢é•¿æœŸ']:
                if field not in [item['field'] for item in hot_topics]:
                    hot_topics.append({
                        'field': field,
                        'reason': f"ç¨³å®šå¢é•¿ (CAGR {analysis['cagr']}%)",
                        'priority': 'medium'
                    })
        
        predictions['hot_topics_prediction'] = sorted(hot_topics, 
                                                    key=lambda x: {'high': 3, 'medium': 2, 'low': 1}[x['priority']], 
                                                    reverse=True)[:10]
        
        # é£é™©è¯„ä¼°
        declining_fields = overall_analysis['declining_fields']['declining_fields']
        risk_fields = []
        
        for field, analysis in declining_fields.items():
            risk_level = 'high' if analysis['declining_score'] >= 3 else 'medium'
            risk_fields.append({
                'field': field,
                'risk_level': risk_level,
                'reason': f"è¿ç»­ä¸‹é™ {analysis['max_consecutive_declines']} å¹´"
            })
        
        predictions['risk_assessment'] = risk_fields
        
        return predictions
    
    def generate_comprehensive_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆè¶‹åŠ¿åˆ†ææŠ¥å‘Š"""
        print("ğŸ“‹ ç”Ÿæˆç»¼åˆç ”ç©¶é¢†åŸŸè¶‹åŠ¿åˆ†ææŠ¥å‘Š...")
        
        # æ‰§è¡Œæ‰€æœ‰åˆ†æ
        overall_analysis = self.analyze_overall_trends()
        conference_analysis = self.analyze_conference_trends()
        predictions = self.generate_predictions(overall_analysis, conference_analysis)
        
        # æ„å»ºå®Œæ•´æŠ¥å‘Š
        comprehensive_report = {
            'generation_time': datetime.now().isoformat(),
            'analysis_period': f"{min(self.analysis_years)}-{max(self.analysis_years)}",
            'data_summary': {
                'total_papers': self.data['basic_statistics']['total_papers'],
                'analyzed_fields': len(self.data['field_analysis']['field_trends']),
                'analyzed_conferences': len(self.data['basic_statistics']['conferences'])
            },
            'overall_trends': overall_analysis,
            'conference_trends': conference_analysis,
            'predictions': predictions,
            'key_insights': self.generate_key_insights(overall_analysis, conference_analysis, predictions)
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.output_dir / 'comprehensive_research_trends.json'
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(comprehensive_report, f, ensure_ascii=False, indent=2)
        
        # ç”Ÿæˆå¯è¯»æŠ¥å‘Š
        self.generate_readable_report(comprehensive_report)
        
        print(f"âœ… ç»¼åˆç ”ç©¶è¶‹åŠ¿åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆï¼š{report_file}")
        return comprehensive_report
    
    def generate_key_insights(self, overall_analysis: Dict, conference_analysis: Dict, predictions: Dict) -> List[str]:
        """ç”Ÿæˆå…³é”®æ´å¯Ÿ"""
        insights = []
        
        # æ€»ä½“è¶‹åŠ¿æ´å¯Ÿ
        emerging_fields = list(overall_analysis['emerging_fields']['emerging_fields'].keys())
        if emerging_fields:
            insights.append(f"ğŸŒŸ æ–°å…´é¢†åŸŸå´›èµ·ï¼š{', '.join(emerging_fields[:3])}å±•ç°å¼ºåŠ²å¢é•¿åŠ¿å¤´")
        
        # ä¸»å¯¼åœ°ä½å˜åŒ–
        dominance = overall_analysis['dominance_shifts']
        if dominance['leadership_changes']:
            latest_change = dominance['leadership_changes'][-1]
            insights.append(f"ğŸ‘‘ é¢†åŸŸä¸»å¯¼æƒè½¬ç§»ï¼š{latest_change['year']}å¹´{latest_change['to']}è¶…è¶Š{latest_change['from']}")
        
        # ä¼šè®®ç‰¹è‰²
        if 'NeuRIPS' in conference_analysis:
            neurips_core = conference_analysis['NeuRIPS']['core_fields'][:2]
            insights.append(f"ğŸ§  NeuRIPSä¸“ä¸šåŒ–ï¼šèšç„¦{', '.join(neurips_core)}ç­‰æ ¸å¿ƒé¢†åŸŸ")
        
        if 'ICLR' in conference_analysis:
            iclr_growth = conference_analysis['ICLR']['growth_analysis']['cagr']
            insights.append(f"ğŸ“ˆ ICLRå‘å±•åŠ¨æ€ï¼šå¹´å¤åˆå¢é•¿ç‡è¾¾{iclr_growth}%")
        
        # é¢„æµ‹æ´å¯Ÿ
        hot_topics = predictions['hot_topics_prediction'][:3]
        if hot_topics:
            hot_names = [topic['field'] for topic in hot_topics]
            insights.append(f"ğŸ”¥ 2025çƒ­ç‚¹é¢„æµ‹ï¼š{', '.join(hot_names)}å°†ç»§ç»­ä¿æŒé«˜çƒ­åº¦")
        
        return insights
    
    def generate_readable_report(self, report: Dict):
        """ç”Ÿæˆå¯è¯»çš„ç ”ç©¶è¶‹åŠ¿åˆ†ææŠ¥å‘Š"""
        readable_content = f"""
# ç ”ç©¶é¢†åŸŸå‘å±•è¶‹åŠ¿æ·±åº¦åˆ†ææŠ¥å‘Š

ç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}
åˆ†ææœŸé—´ï¼š{report['analysis_period']}
æ•°æ®æ¥æºï¼š{report['data_summary']['total_papers']:,}ç¯‡AIé¡¶çº§ä¼šè®®è®ºæ–‡

## ğŸ¯ æ ¸å¿ƒæ´å¯Ÿ

{chr(10).join(f"â€¢ {insight}" for insight in report['key_insights'])}

## ğŸ“Š æ€»ä½“è¶‹åŠ¿åˆ†æ

### æ–°å…´é¢†åŸŸè¯†åˆ«
"""
        
        # æ–°å…´é¢†åŸŸåˆ†æ
        emerging = report['overall_trends']['emerging_fields']['emerging_fields']
        if emerging:
            readable_content += "\n**å¿«é€Ÿå´›èµ·çš„ç ”ç©¶é¢†åŸŸï¼š**\n"
            for i, (field, data) in enumerate(emerging.items(), 1):
                growth_rate = data['growth_rate']
                growth_str = f"{growth_rate}%" if growth_rate != 'inf' else "è¶…é«˜é€Ÿ"
                readable_content += f"{i}. **{field}**\n"
                readable_content += f"   - å¢é•¿å€æ•°ï¼š{data['growth_multiple']}\n"
                readable_content += f"   - å¢é•¿ç‡ï¼š{growth_str}\n"
                readable_content += f"   - æ–°å…´è¯„åˆ†ï¼š{data['emerging_score']}/4\n\n"
        
        # é¢†åŸŸä¸»å¯¼åœ°ä½åˆ†æ
        dominance = report['overall_trends']['dominance_shifts']
        readable_content += "\n### é¢†åŸŸä¸»å¯¼åœ°ä½æ¼”å˜\n\n"
        
        most_dominant = dominance['most_dominant_overall']
        readable_content += f"**æ€»ä½“æœ€ä¸»å¯¼é¢†åŸŸï¼š**\n"
        readable_content += f"- æŒ‰è®ºæ–‡æ•°é‡ï¼š{most_dominant['by_total_papers']['field']} ({most_dominant['by_total_papers']['papers']:,}ç¯‡)\n"
        readable_content += f"- æŒ‰å¹³å‡ä»½é¢ï¼š{most_dominant['by_average_share']['field']} ({most_dominant['by_average_share']['avg_share']}%)\n\n"
        
        if dominance['leadership_changes']:
            readable_content += "**ä¸»å¯¼æƒå˜è¿å†å²ï¼š**\n"
            for change in dominance['leadership_changes']:
                readable_content += f"- {change['year']}å¹´ï¼š{change['to']}å–ä»£{change['from']}æˆä¸ºä¸»å¯¼é¢†åŸŸ\n"
        
        # è¶‹åŠ¿æ¨¡å¼åˆ†æ
        readable_content += "\n### å‘å±•è¶‹åŠ¿æ¨¡å¼åˆ†ç±»\n\n"
        trend_patterns = report['overall_trends']['trend_patterns']
        
        pattern_groups = {}
        for field, pattern in trend_patterns.items():
            trend_type = pattern['trend_type']
            if trend_type not in pattern_groups:
                pattern_groups[trend_type] = []
            pattern_groups[trend_type].append(field)
        
        for pattern_type, fields in pattern_groups.items():
            readable_content += f"**{pattern_type}é¢†åŸŸï¼š**\n"
            for field in fields:
                pattern_data = trend_patterns[field]
                readable_content += f"- {field}ï¼ˆæ–œç‡ï¼š{pattern_data['linear_slope']}, RÂ²ï¼š{pattern_data['r_squared']}ï¼‰\n"
            readable_content += "\n"
        
        ## ä¼šè®®ä¸“ä¸šåŒ–åˆ†æ
        readable_content += "\n## ğŸ›ï¸ å„ä¼šè®®ç ”ç©¶é¢†åŸŸç‰¹è‰²åˆ†æ\n\n"
        
        conferences = ['NeuRIPS', 'ICLR', 'AAAI']
        for conf in conferences:
            if conf in report['conference_trends']:
                conf_data = report['conference_trends'][conf]
                readable_content += f"### {conf} ä¸“ä¸šåŒ–åˆ†æ\n\n"
                
                # æ ¸å¿ƒé¢†åŸŸ
                core_fields = conf_data['core_fields']
                readable_content += f"**æ ¸å¿ƒä¸“ä¸šé¢†åŸŸï¼š**\n"
                for field in core_fields:
                    spec_score = conf_data['specialization_scores'][field]
                    readable_content += f"- {field}ï¼ˆä¸“ä¸šåŒ–æŒ‡æ•°ï¼š{spec_score:.3f}ï¼‰\n"
                
                # å¢é•¿åˆ†æ
                growth = conf_data['growth_analysis']
                readable_content += f"\n**å‘å±•åŠ¨æ€ï¼š**\n"
                readable_content += f"- å¹´å¤åˆå¢é•¿ç‡ï¼š{growth['cagr']}%\n"
                readable_content += f"- å¢é•¿é˜¶æ®µï¼š{growth['growth_phase']}\n"
                readable_content += f"- æ€»è®ºæ–‡æ•°ï¼š{conf_data['total_papers']:,}ç¯‡\n"
                
                # é¢†åŸŸé›†ä¸­åº¦
                concentration = conf_data['field_concentration']
                readable_content += f"- é¢†åŸŸé›†ä¸­åº¦ï¼š{concentration['concentration_level']}ï¼ˆHHI: {concentration['hhi']}ï¼‰\n\n"
        
        # ä¼šè®®æ¯”è¾ƒåˆ†æ
        if 'comparative_analysis' in report['conference_trends']:
            comp = report['conference_trends']['comparative_analysis']
            readable_content += "### ä¼šè®®é—´æ¯”è¾ƒåˆ†æ\n\n"
            
            readable_content += "**å¤šæ ·æ€§æ’åï¼š**\n"
            for i, (conf, score) in enumerate(comp['diversity_ranking'].items(), 1):
                readable_content += f"{i}. {conf}ï¼ˆåŸºå°¼ç³»æ•°ï¼š{score}ï¼‰\n"
            
            readable_content += "\n**å¢é•¿é€Ÿåº¦æ’åï¼š**\n"
            for i, (conf, cagr) in enumerate(comp['growth_ranking'].items(), 1):
                readable_content += f"{i}. {conf}ï¼ˆCAGRï¼š{cagr}%ï¼‰\n"
            
            readable_content += "\n**ä¸“ä¸šåŒ–ç‹¬ç‰¹æ€§ï¼š**\n"
            for conf, unique_fields in comp['specialization_uniqueness'].items():
                if unique_fields:
                    readable_content += f"- {conf}ï¼š{', '.join(unique_fields)}\n"
        
        # é¢„æµ‹åˆ†æ
        readable_content += "\n## ğŸ”® å‘å±•è¶‹åŠ¿é¢„æµ‹\n\n"
        
        # 2025å¹´é¢„æµ‹
        forecasts = report['predictions']['2025_forecasts']
        readable_content += "### 2025å¹´é¢†åŸŸè§„æ¨¡é¢„æµ‹\n\n"
        
        # æŒ‰é¢„æµ‹è®ºæ–‡æ•°æ’åº
        sorted_forecasts = sorted(forecasts.items(), key=lambda x: x[1]['predicted_papers'], reverse=True)
        
        for i, (field, forecast) in enumerate(sorted_forecasts[:10], 1):
            confidence_icon = {"high": "ğŸŸ¢", "medium": "ğŸŸ¡", "low": "ğŸ”´"}[forecast['confidence_level']]
            readable_content += f"{i}. **{field}**ï¼š{forecast['predicted_papers']:,}ç¯‡ {confidence_icon}\n"
            readable_content += f"   - é¢„æµ‹åŸºç¡€ï¼š{forecast['trend_basis']}\n"
            readable_content += f"   - ç½®ä¿¡æ°´å¹³ï¼š{forecast['confidence_level']}\n\n"
        
        # çƒ­ç‚¹é¢„æµ‹
        hot_topics = report['predictions']['hot_topics_prediction']
        readable_content += "### ç ”ç©¶çƒ­ç‚¹é¢„æµ‹\n\n"
        
        for i, topic in enumerate(hot_topics, 1):
            priority_icon = {"high": "ğŸ”¥", "medium": "â­", "low": "ğŸ’¡"}[topic['priority']]
            readable_content += f"{i}. **{topic['field']}** {priority_icon}\n"
            readable_content += f"   - é¢„æµ‹ç†ç”±ï¼š{topic['reason']}\n\n"
        
        # é£é™©è¯„ä¼°
        risks = report['predictions']['risk_assessment']
        if risks:
            readable_content += "### è¡°é€€é£é™©è¯„ä¼°\n\n"
            for risk in risks:
                risk_icon = {"high": "âš ï¸", "medium": "âš¡"}[risk['risk_level']]
                readable_content += f"- **{risk['field']}** {risk_icon}\n"
                readable_content += f"  - é£é™©ç­‰çº§ï¼š{risk['risk_level']}\n"
                readable_content += f"  - é£é™©åŸå› ï¼š{risk['reason']}\n\n"
        
        # å»ºè®®ä¸ç»“è®º
        readable_content += """
## ğŸ’¡ ç­–ç•¥å»ºè®®

### ç ”ç©¶æŠ•èµ„æ–¹å‘
1. **é‡ç‚¹å…³æ³¨æ–°å…´é¢†åŸŸ**ï¼šManufacturingã€Medical Diagnosisç­‰å±•ç°å·¨å¤§æ½œåŠ›
2. **ç¨³å®šæŠ•èµ„æˆç†Ÿé¢†åŸŸ**ï¼šEducational Technologyã€Content Creationä¿æŒç¨³å®šå¢é•¿
3. **å…³æ³¨æŠ€æœ¯èåˆ**ï¼šè·¨é¢†åŸŸç ”ç©¶æˆä¸ºæ–°è¶‹åŠ¿

### ä¼šè®®é€‰æ‹©ç­–ç•¥
1. **NeuRIPS**ï¼šé€‚åˆæ·±åº¦å­¦ä¹ å’Œä¼˜åŒ–ç›¸å…³ç ”ç©¶
2. **ICLR**ï¼šè¡¨ç°å‹ç®—æ³•å’Œè¡¨ç¤ºå­¦ä¹ çš„é¦–é€‰å¹³å°
3. **AAAI**ï¼šé€šç”¨äººå·¥æ™ºèƒ½å’Œåº”ç”¨å¯¼å‘ç ”ç©¶çš„ç†æƒ³é€‰æ‹©

### é£é™©è§„é¿
1. **è°¨æ…æŠ•èµ„è¡°é€€é¢†åŸŸ**ï¼šå…³æ³¨Social Mediaç­‰é¢†åŸŸçš„å‘å±•å˜åŒ–
2. **å¤šå…ƒåŒ–ç ”ç©¶ç»„åˆ**ï¼šé¿å…è¿‡åº¦é›†ä¸­åœ¨å•ä¸€é¢†åŸŸ
3. **ä¿æŒæŠ€æœ¯æ•æ„Ÿæ€§**ï¼šåŠæ—¶è·Ÿè¿›æ–°å…´æŠ€æœ¯è¶‹åŠ¿

---
*æœ¬æŠ¥å‘ŠåŸºäº31,244ç¯‡AIé¡¶çº§ä¼šè®®è®ºæ–‡çš„æ·±åº¦åˆ†æï¼Œæ¶µç›–2018-2023å¹´å®Œæ•´æ•°æ®*
"""
        
        # ä¿å­˜å¯è¯»æŠ¥å‘Š
        readable_file = self.output_dir / 'research_trends_analysis_readable.md'
        with open(readable_file, 'w', encoding='utf-8') as f:
            f.write(readable_content)
        
        print(f"ğŸ“ å¯è¯»æŠ¥å‘Šå·²ç”Ÿæˆï¼š{readable_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨ç ”ç©¶é¢†åŸŸå‘å±•è¶‹åŠ¿æ·±åº¦åˆ†æ...")
    
    analyzer = ResearchTrendsAnalyzer()
    report = analyzer.generate_comprehensive_report()
    
    print("\n" + "="*60)
    print("ğŸ“Š ç ”ç©¶é¢†åŸŸè¶‹åŠ¿åˆ†æå®Œæˆï¼")
    print(f"ğŸ“ æŠ¥å‘Šä¿å­˜ä½ç½®ï¼šoutputs/research_trends/")
    print("\nğŸ” æ ¸å¿ƒæ´å¯Ÿï¼š")
    for insight in report['key_insights']:
        print(f"  â€¢ {insight}")
    
    return report

if __name__ == "__main__":
    main()