#!/usr/bin/env python3
"""
æ·±åº¦è¶‹åŠ¿åˆ†æå™¨
åˆ†æç ”ç©¶é¢†åŸŸã€åº”ç”¨åœºæ™¯ã€æŠ€æœ¯å‘å±•ã€ä»»åŠ¡åœºæ™¯çš„å‘å±•è¶‹åŠ¿
"""

import json
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Tuple
import matplotlib.pyplot as plt
import seaborn as sns

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

class TrendAnalyzer:
    """æ·±åº¦è¶‹åŠ¿åˆ†æå™¨"""
    
    def __init__(self, data_path: str = "outputs/analysis/comprehensive_analysis.json"):
        self.data_path = Path(data_path)
        self.data = self.load_data()
        self.output_dir = Path("outputs/trend_analysis")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
    def load_data(self) -> Dict:
        """åŠ è½½åˆ†ææ•°æ®"""
        with open(self.data_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def analyze_research_fields_trends(self) -> Dict[str, Any]:
        """åˆ†æç ”ç©¶é¢†åŸŸå‘å±•è¶‹åŠ¿"""
        print("ğŸ” åˆ†æç ”ç©¶é¢†åŸŸå‘å±•è¶‹åŠ¿...")
        
        field_trends = self.data['field_analysis']['field_trends']
        years = list(range(2018, 2025))
        
        # è®¡ç®—å¢é•¿ç‡å’Œè¶‹åŠ¿
        trends_analysis = {}
        
        for field, yearly_data in field_trends.items():
            # æå–å¹´åº¦æ•°æ®
            values = [yearly_data.get(str(year), 0) for year in years]
            
            # è®¡ç®—å„ç§è¶‹åŠ¿æŒ‡æ ‡
            total_papers = sum(values)
            peak_year = years[values.index(max(values))]
            peak_value = max(values)
            
            # è®¡ç®—å¢é•¿ç‡ (2018-2023, æ’é™¤2024å¹´å› ä¸ºæ•°æ®ä¸å®Œæ•´)
            early_avg = np.mean(values[:3])  # 2018-2020
            recent_avg = np.mean(values[3:6])  # 2021-2023
            growth_rate = ((recent_avg - early_avg) / early_avg * 100) if early_avg > 0 else 0
            
            # è®¡ç®—è¶‹åŠ¿ç³»æ•° (çº¿æ€§å›å½’æ–œç‡)
            x = np.array(years[:-1])  # æ’é™¤2024
            y = np.array(values[:-1])
            trend_coeff = np.polyfit(x, y, 1)[0] if len(y) > 1 else 0
            
            # åˆ¤æ–­è¶‹åŠ¿ç±»å‹
            if trend_coeff > 50:
                trend_type = "å¼ºåŠ²å¢é•¿"
            elif trend_coeff > 20:
                trend_type = "ç¨³æ­¥å¢é•¿"
            elif trend_coeff > -20:
                trend_type = "åŸºæœ¬ç¨³å®š"
            else:
                trend_type = "ä¸‹é™è¶‹åŠ¿"
                
            trends_analysis[field] = {
                'total_papers': total_papers,
                'peak_year': peak_year,
                'peak_value': peak_value,
                'growth_rate': round(growth_rate, 1),
                'trend_coefficient': round(trend_coeff, 2),
                'trend_type': trend_type,
                'yearly_values': values,
                'market_share_2023': round(yearly_data.get('2023', 0) / sum(yearly_data.get('2023', 0) for yearly_data in field_trends.values()) * 100, 2)
            }
        
        return trends_analysis
    
    def analyze_application_scenarios_trends(self) -> Dict[str, Any]:
        """åˆ†æåº”ç”¨åœºæ™¯å‘å±•è¶‹åŠ¿"""
        print("ğŸ¯ åˆ†æåº”ç”¨åœºæ™¯å‘å±•è¶‹åŠ¿...")
        
        scenario_trends = self.data['task_scenario_analysis']['scenario_yearly_trends']
        years = list(range(2018, 2025))
        
        scenarios_analysis = {}
        
        for scenario, yearly_data in scenario_trends.items():
            if scenario == "General Research":  # è·³è¿‡é€šç”¨ç ”ç©¶
                continue
                
            values = [yearly_data.get(str(year), 0) for year in years]
            
            # è®¡ç®—æˆç†Ÿåº¦æŒ‡æ ‡
            total_papers = sum(values)
            consistency = np.std(values[:-1]) / np.mean(values[:-1]) if np.mean(values[:-1]) > 0 else 0  # å˜å¼‚ç³»æ•°
            
            # è®¡ç®—å‘å±•é˜¶æ®µ
            early_sum = sum(values[:2])  # 2018-2019
            middle_sum = sum(values[2:5])  # 2020-2022
            recent_sum = sum(values[5:7])  # 2023-2024
            
            if early_sum < 100 and recent_sum > middle_sum:
                development_stage = "æ–°å…´é¢†åŸŸ"
            elif middle_sum > early_sum * 2 and recent_sum > middle_sum:
                development_stage = "å¿«é€Ÿå‘å±•"
            elif consistency < 0.3:
                development_stage = "æˆç†Ÿç¨³å®š"
            else:
                development_stage = "æ³¢åŠ¨è°ƒæ•´"
            
            # è®¡ç®—å¹´å¤åˆå¢é•¿ç‡ (CAGR)
            if values[0] > 0 and values[5] > 0:  # 2018 to 2023
                cagr = (pow(values[5] / values[0], 1/5) - 1) * 100
            else:
                cagr = 0
                
            scenarios_analysis[scenario] = {
                'total_papers': total_papers,
                'development_stage': development_stage,
                'cagr_2018_2023': round(cagr, 1),
                'consistency_score': round(1 - consistency, 2),  # ä¸€è‡´æ€§è¯„åˆ†
                'yearly_values': values,
                'recent_momentum': values[5] - values[4] if len(values) > 5 else 0  # 2023 vs 2022
            }
        
        return scenarios_analysis
    
    def analyze_technology_trends(self) -> Dict[str, Any]:
        """åˆ†ææŠ€æœ¯å‘å±•è¶‹åŠ¿"""
        print("ğŸ’» åˆ†ææŠ€æœ¯å‘å±•è¶‹åŠ¿...")
        
        # åŸºäºå…³é”®è¯åˆ†ææŠ€æœ¯è¶‹åŠ¿
        keywords = self.data['keyword_analysis']['top_keywords']
        
        # å®šä¹‰æŠ€æœ¯ç±»åˆ«
        tech_categories = {
            'Deep Learning': ['deep', 'neural', 'networks', 'cnn', 'rnn', 'transformer'],
            'Machine Learning': ['learning', 'training', 'algorithm', 'model', 'supervised'],
            'Optimization': ['optimization', 'gradient', 'sgd', 'adam', 'loss'],
            'Graph Technology': ['graph', 'node', 'edge', 'network', 'social'],
            'Computer Vision': ['image', 'vision', 'visual', 'object', 'detection'],
            'Natural Language': ['language', 'text', 'nlp', 'word', 'semantic'],
            'Reinforcement Learning': ['reinforcement', 'reward', 'policy', 'agent', 'environment'],
            'Generative Models': ['generative', 'gan', 'vae', 'diffusion', 'generation']
        }
        
        tech_scores = {}
        for category, category_keywords in tech_categories.items():
            score = sum(keywords.get(keyword, 0) for keyword in category_keywords)
            tech_scores[category] = score
        
        # ä»»åŠ¡ç±»å‹æ¼”åŒ–è¶‹åŠ¿
        task_evolution = self.data['cross_analysis']['yearly_task_evolution']
        
        return {
            'technology_popularity': tech_scores,
            'task_type_evolution': task_evolution
        }
    
    def analyze_task_scenarios_trends(self) -> Dict[str, Any]:
        """åˆ†æä»»åŠ¡åœºæ™¯å‘å±•è¶‹åŠ¿"""
        print("âš™ï¸ åˆ†æä»»åŠ¡åœºæ™¯å‘å±•è¶‹åŠ¿...")
        
        task_evolution = self.data['cross_analysis']['yearly_task_evolution']
        years = list(range(2018, 2025))
        
        task_trends = {}
        
        for task_type, yearly_ratios in task_evolution.items():
            values = [yearly_ratios.get(str(year), 0) * 100 for year in years[:-1]]  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
            
            # è®¡ç®—è¶‹åŠ¿
            trend_direction = "ä¸Šå‡" if values[-1] > values[0] else "ä¸‹é™"
            volatility = np.std(values)
            
            # è®¡ç®—åœ¨ä¸åŒæ—¶æœŸçš„é‡è¦æ€§
            early_importance = np.mean(values[:2])
            recent_importance = np.mean(values[-2:])
            
            task_trends[task_type] = {
                'trend_direction': trend_direction,
                'volatility': round(volatility, 3),
                'early_importance': round(early_importance, 2),
                'recent_importance': round(recent_importance, 2),
                'importance_change': round(recent_importance - early_importance, 2),
                'yearly_percentages': values
            }
        
        return task_trends
    
    def generate_trend_insights(self, field_trends: Dict, scenario_trends: Dict, 
                              tech_trends: Dict, task_trends: Dict) -> List[str]:
        """ç”Ÿæˆè¶‹åŠ¿æ´å¯Ÿ"""
        insights = []
        
        # ç ”ç©¶é¢†åŸŸæ´å¯Ÿ
        fastest_growing = max(field_trends.items(), key=lambda x: x[1]['growth_rate'])
        insights.append(f"ğŸ“ˆ å¢é•¿æœ€å¿«çš„ç ”ç©¶é¢†åŸŸï¼š{fastest_growing[0]}ï¼ˆå¢é•¿ç‡{fastest_growing[1]['growth_rate']}%ï¼‰")
        
        # åº”ç”¨åœºæ™¯æ´å¯Ÿ
        emerging_scenarios = [name for name, data in scenario_trends.items() 
                             if data['development_stage'] == 'æ–°å…´é¢†åŸŸ']
        if emerging_scenarios:
            insights.append(f"ğŸŒŸ æ–°å…´åº”ç”¨åœºæ™¯ï¼š{', '.join(emerging_scenarios)}")
        
        # æŠ€æœ¯è¶‹åŠ¿æ´å¯Ÿ
        top_tech = max(tech_trends['technology_popularity'].items(), key=lambda x: x[1])
        insights.append(f"ğŸ”¥ æœ€çƒ­é—¨æŠ€æœ¯ï¼š{top_tech[0]}ï¼ˆæåŠæ¬¡æ•°{top_tech[1]:,}ï¼‰")
        
        # ä»»åŠ¡ç±»å‹æ´å¯Ÿ
        rising_tasks = [task for task, data in task_trends.items() 
                       if data['importance_change'] > 1]
        if rising_tasks:
            insights.append(f"â¬†ï¸ é‡è¦æ€§ä¸Šå‡çš„ä»»åŠ¡ç±»å‹ï¼š{', '.join(rising_tasks)}")
        
        return insights
    
    def generate_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆå®Œæ•´çš„è¶‹åŠ¿åˆ†ææŠ¥å‘Š"""
        print("ğŸ“Š ç”Ÿæˆè¶‹åŠ¿åˆ†ææŠ¥å‘Š...")
        
        # æ‰§è¡Œå„é¡¹åˆ†æ
        field_trends = self.analyze_research_fields_trends()
        scenario_trends = self.analyze_application_scenarios_trends()
        tech_trends = self.analyze_technology_trends()
        task_trends = self.analyze_task_scenarios_trends()
        
        # ç”Ÿæˆæ´å¯Ÿ
        insights = self.generate_trend_insights(field_trends, scenario_trends, tech_trends, task_trends)
        
        # æ„å»ºå®Œæ•´æŠ¥å‘Š
        report = {
            'generation_time': datetime.now().isoformat(),
            'research_fields_trends': field_trends,
            'application_scenarios_trends': scenario_trends,
            'technology_trends': tech_trends,
            'task_scenarios_trends': task_trends,
            'key_insights': insights,
            'summary_statistics': {
                'total_research_fields': len(field_trends),
                'total_application_scenarios': len(scenario_trends),
                'analysis_period': "2018-2024",
                'fastest_growing_field': max(field_trends.items(), key=lambda x: x[1]['growth_rate'])[0],
                'most_stable_field': min(field_trends.items(), key=lambda x: abs(x[1]['trend_coefficient']))[0]
            }
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.output_dir / 'trend_analysis_report.json'
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # ç”Ÿæˆå¯è¯»æŠ¥å‘Š
        self.generate_readable_report(report)
        
        print(f"âœ… è¶‹åŠ¿åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆï¼š{report_file}")
        return report
    
    def generate_readable_report(self, report: Dict):
        """ç”Ÿæˆå¯è¯»çš„è¶‹åŠ¿åˆ†ææŠ¥å‘Š"""
        readable_report = f"""
# AIä¼šè®®è®ºæ–‡å‘å±•è¶‹åŠ¿æ·±åº¦åˆ†ææŠ¥å‘Š

ç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}
åˆ†ææœŸé—´ï¼š2018-2024å¹´
æ•°æ®æ¥æºï¼š31,244ç¯‡AIé¡¶çº§ä¼šè®®è®ºæ–‡

## ğŸ“Š æ ¸å¿ƒå‘ç°

{chr(10).join(f"â€¢ {insight}" for insight in report['key_insights'])}

## ğŸ”¬ ç ”ç©¶é¢†åŸŸå‘å±•è¶‹åŠ¿

### å¤´éƒ¨é¢†åŸŸï¼ˆè®ºæ–‡æ•°é‡Top 5ï¼‰
"""
        
        # ç ”ç©¶é¢†åŸŸåˆ†æ
        field_trends = report['research_fields_trends']
        top_fields = sorted(field_trends.items(), key=lambda x: x[1]['total_papers'], reverse=True)[:5]
        
        for i, (field, data) in enumerate(top_fields, 1):
            readable_report += f"""
{i}. **{field}**
   - æ€»è®ºæ–‡æ•°ï¼š{data['total_papers']:,}ç¯‡
   - å‘å±•è¶‹åŠ¿ï¼š{data['trend_type']}
   - å¢é•¿ç‡ï¼š{data['growth_rate']}%
   - å³°å€¼å¹´ä»½ï¼š{data['peak_year']}å¹´ï¼ˆ{data['peak_value']:,}ç¯‡ï¼‰
   - å¸‚åœºä»½é¢ï¼š{data['market_share_2023']}%ï¼ˆ2023å¹´ï¼‰
"""

        readable_report += f"""
### å¢é•¿æœ€å¿«é¢†åŸŸï¼ˆTop 3ï¼‰
"""
        fastest_growing = sorted(field_trends.items(), key=lambda x: x[1]['growth_rate'], reverse=True)[:3]
        for i, (field, data) in enumerate(fastest_growing, 1):
            readable_report += f"{i}. {field}ï¼š{data['growth_rate']}%å¢é•¿\n"

        # åº”ç”¨åœºæ™¯åˆ†æ
        readable_report += f"""
## ğŸ¯ åº”ç”¨åœºæ™¯å‘å±•è¶‹åŠ¿

### å‘å±•é˜¶æ®µåˆ†å¸ƒ
"""
        scenario_trends = report['application_scenarios_trends']
        stage_distribution = {}
        for scenario, data in scenario_trends.items():
            stage = data['development_stage']
            if stage not in stage_distribution:
                stage_distribution[stage] = []
            stage_distribution[stage].append(scenario)
        
        for stage, scenarios in stage_distribution.items():
            readable_report += f"- **{stage}**ï¼š{', '.join(scenarios)}\n"

        readable_report += f"""
### å¤åˆå¢é•¿ç‡æœ€é«˜åœºæ™¯ï¼ˆCAGR Top 5ï¼‰
"""
        top_cagr = sorted(scenario_trends.items(), key=lambda x: x[1]['cagr_2018_2023'], reverse=True)[:5]
        for i, (scenario, data) in enumerate(top_cagr, 1):
            readable_report += f"{i}. {scenario}ï¼š{data['cagr_2018_2023']}%\n"

        # æŠ€æœ¯è¶‹åŠ¿åˆ†æ
        readable_report += f"""
## ğŸ’» æŠ€æœ¯å‘å±•è¶‹åŠ¿

### æŠ€æœ¯çƒ­åº¦æ’è¡Œï¼ˆåŸºäºå…³é”®è¯æåŠæ¬¡æ•°ï¼‰
"""
        tech_popularity = report['technology_trends']['technology_popularity']
        sorted_tech = sorted(tech_popularity.items(), key=lambda x: x[1], reverse=True)
        for i, (tech, score) in enumerate(sorted_tech, 1):
            readable_report += f"{i}. {tech}ï¼š{score:,}æ¬¡æåŠ\n"

        # ä»»åŠ¡ç±»å‹è¶‹åŠ¿
        readable_report += f"""
## âš™ï¸ ä»»åŠ¡ç±»å‹æ¼”åŒ–è¶‹åŠ¿

### é‡è¦æ€§å˜åŒ–åˆ†æ
"""
        task_trends = report['task_scenarios_trends']
        for task_type, data in task_trends.items():
            change_indicator = "ğŸ“ˆ" if data['importance_change'] > 0 else "ğŸ“‰"
            readable_report += f"- **{task_type}** {change_indicator}\n"
            readable_report += f"  - æ—©æœŸé‡è¦æ€§ï¼š{data['early_importance']}%\n"
            readable_report += f"  - è¿‘æœŸé‡è¦æ€§ï¼š{data['recent_importance']}%\n"
            readable_report += f"  - å˜åŒ–å¹…åº¦ï¼š{data['importance_change']:+.2f}%\n\n"

        readable_report += f"""
## ğŸ“ˆ é¢„æµ‹ä¸å»ºè®®

### å€¼å¾—å…³æ³¨çš„å‘å±•æ–¹å‘
1. **æ–°å…´é¢†åŸŸæŠ•èµ„**ï¼šManufacturingã€Medical Diagnosisç­‰æ–°å…´åœºæ™¯å±•ç°å¼ºåŠ²å¢é•¿æ½œåŠ›
2. **æŠ€æœ¯èåˆè¶‹åŠ¿**ï¼šDeep Learningä¸å…¶ä»–æŠ€æœ¯é¢†åŸŸçš„äº¤å‰èåˆåŠ é€Ÿ
3. **åº”ç”¨åœºæ™¯æ‰©å±•**ï¼šä»å­¦æœ¯ç ”ç©¶å‘å®é™…åº”ç”¨åœºæ™¯è½¬ç§»çš„è¶‹åŠ¿æ˜æ˜¾
4. **ä»»åŠ¡ç±»å‹å¤šæ ·åŒ–**ï¼šGeneration Taskså’ŒOptimization Tasksé‡è¦æ€§æŒç»­ä¸Šå‡

### ç ”ç©¶çƒ­ç‚¹é¢„æµ‹
åŸºäºå½“å‰è¶‹åŠ¿ï¼Œé¢„è®¡æœªæ¥2-3å¹´å†…ä»¥ä¸‹é¢†åŸŸå°†ç»§ç»­ä¿æŒé«˜çƒ­åº¦ï¼š
- Educational Technologyï¼ˆæ•™è‚²æŠ€æœ¯ï¼‰
- Content Creationï¼ˆå†…å®¹åˆ›ä½œï¼‰
- Medical Diagnosisï¼ˆåŒ»ç–—è¯Šæ–­ï¼‰
- Autonomous Drivingï¼ˆè‡ªåŠ¨é©¾é©¶ï¼‰

---
*æŠ¥å‘ŠåŸºäº{report['summary_statistics']['total_research_fields']}ä¸ªç ”ç©¶é¢†åŸŸå’Œ{report['summary_statistics']['total_application_scenarios']}ä¸ªåº”ç”¨åœºæ™¯çš„æ·±åº¦åˆ†æ*
"""
        
        # ä¿å­˜å¯è¯»æŠ¥å‘Š
        readable_file = self.output_dir / 'trend_analysis_readable.md'
        with open(readable_file, 'w', encoding='utf-8') as f:
            f.write(readable_report)
        
        print(f"ğŸ“ å¯è¯»æŠ¥å‘Šå·²ç”Ÿæˆï¼š{readable_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨æ·±åº¦è¶‹åŠ¿åˆ†æ...")
    
    analyzer = TrendAnalyzer()
    report = analyzer.generate_report()
    
    print("\n" + "="*50)
    print("ğŸ“Š è¶‹åŠ¿åˆ†æå®Œæˆï¼")
    print(f"ğŸ“ æŠ¥å‘Šä¿å­˜ä½ç½®ï¼šoutputs/trend_analysis/")
    print("\nğŸ” æ ¸å¿ƒæ´å¯Ÿï¼š")
    for insight in report['key_insights']:
        print(f"  â€¢ {insight}")
    
    return report

if __name__ == "__main__":
    main()