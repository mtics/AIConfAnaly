# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This is a conference paper analysis tool that scrapes and analyzes research papers from major AI/ML conferences (ICML, NeuRIPS, ICLR, AAAI, IJCAI). It performs automated scraping, NLP-based analysis, field classification, and generates comprehensive interactive visualizations.

## Development Commands

### Setup and Dependencies
```bash
# Install dependencies via pip
pip install -r requirements.txt

# Or via conda
conda env create -f environment.yml
conda activate ConfAnalysis
```

### Main Commands
```bash
# Full analysis pipeline (recommended)
python main_new.py

# Or using module approach
python -m conf_analysis.main

# Generate comprehensive dashboard
python -c "from conf_analysis.main import main; main()"
```

### Cleanup and Maintenance
```bash
# Clean up redundant files (run once after reorganization)
python tools/utilities/cleanup_project.py
```

## Architecture Overview

### Final Optimized Structure

```
ConfAnalysis/
â”œâ”€â”€ conf_analysis/          # ğŸ—ï¸ Core analysis system
â”‚   â”œâ”€â”€ core/              # Core components  
â”‚   â”‚   â”œâ”€â”€ analyzer.py    # Enhanced analyzer (70+ scenarios, 25+ tech trends)
â”‚   â”‚   â”œâ”€â”€ scrapers/      # Conference-specific scrapers
â”‚   â”‚   â”œâ”€â”€ models/        # Data models
â”‚   â”‚   â”œâ”€â”€ services/      # Business services
â”‚   â”‚   â”œâ”€â”€ utils/         # Utility functions
â”‚   â”‚   â”œâ”€â”€ database/      # Database modules
â”‚   â”‚   â””â”€â”€ embeddings/    # Vector encoding
â”‚   â”œâ”€â”€ docs/             # Project documentation
â”‚   â””â”€â”€ main.py           # Main analysis program
â”œâ”€â”€ tools/                 # ğŸ”§ Utility tools (organized)
â”‚   â”œâ”€â”€ utilities/         # Helper utilities
â”‚   â”œâ”€â”€ data_generators/   # Data generation scripts
â”‚   â””â”€â”€ visualization_generators/  # Chart generators
â”œâ”€â”€ frontend/             # ğŸ¨ Web interface (cleaned & optimized)
â”‚   â”œâ”€â”€ unified_analysis_dashboard.html  # â­ Main dashboard
â”‚   â””â”€â”€ unified_analysis_report.html     # â­ Generated report
â”œâ”€â”€ outputs/              # ğŸ“Š Analysis results
â”œâ”€â”€ tests/               # ğŸ§ª Test files
â”œâ”€â”€ main_new.py          # ğŸšª Main entry point
â””â”€â”€ PROJECT_STRUCTURE.md # ğŸ“ Structure documentation
```

### Core Components

**Core Analysis (`conf_analysis/core/`)**
- `analyzer.py`: UnifiedAnalyzer with enhanced field classification
- `scrapers/`: Conference-specific scraping implementations
- `models/`: Data models for papers and analysis results
- `utils/`: Configuration, text processing, and database utilities

**Tools (`tools/`)**
- `utilities/`: Legacy analyzer scripts and cleanup tools
- `visualization_generators/`: Chart and dashboard generators
- `data_generators/`: Data processing and analysis scripts

### Data Flow
1. **Scraping**: Conference-specific scrapers extract paper metadata â†’ JSON files in `data/raw/`
2. **Processing**: DataProcessor cleans text, extracts keywords â†’ CSV in `data/processed/`
3. **Field Classification**: FieldExtractor categorizes papers by research area
4. **Analysis**: Statistical analysis and trend identification
5. **Visualization**: Charts, word clouds, interactive dashboard â†’ `results/figures/`

### Key Design Patterns
- **Strategy Pattern**: Different scraper implementations for each conference
- **Template Method**: BaseScraper defines common workflow, subclasses implement specifics
- **Pipeline Architecture**: Sequential data processing stages with intermediate outputs
- **Configuration-driven**: Centralized settings in config.py for easy customization

## Research Field Classification

The system automatically classifies papers into research areas using keyword matching and ML techniques. Fields include Computer Vision, NLP, Reinforcement Learning, Deep Learning Theory, Optimization, etc. Keywords are defined in `analysis/field_extractor.py`.

## Adding New Conferences

1. Create new scraper class inheriting from `BaseScraper`
2. Implement `get_papers_for_year(year)` method
3. Add conference configuration to `CONFERENCES` dict in `config.py`
4. Import in `scrapers/__init__.py`

## Data Storage Structure

- `data/raw/`: JSON files with scraped paper data (one per conference/year)
- `data/processed/`: Processed CSV files with extracted features  
- `results/`: Analysis outputs and interactive dashboard
  - `ai_conference_dashboard.html`: Main interactive dashboard
  - `figures/`: Static visualization files

## Output Files

The main output is an interactive HTML dashboard located at:
- `results/ai_conference_dashboard.html`

This dashboard provides comprehensive analysis including:
- Publication trends across conferences and years
- Detailed research field classification (130+ categories)
- Interactive visualizations with hover details
- Conference comparison and field evolution analysis