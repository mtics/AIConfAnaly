#!/usr/bin/env python3
"""
AIä¼šè®®è®ºæ–‡åˆ†æç³»ç»Ÿ - å®Œæ•´é›†æˆä¸»å…¥å£
ç«¯åˆ°ç«¯å·¥ä½œæµï¼šè®ºæ–‡çˆ¬å– â†’ PDFä¸‹è½½ â†’ å‘é‡ç¼–ç  â†’ Milvuså­˜å‚¨ â†’ åˆ†ææŠ¥å‘Š
"""

import sys
import os
import asyncio
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import logging
import json
from datetime import datetime

# ç¡®ä¿æ­£ç¡®çš„å¯¼å…¥è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

# æ ¸å¿ƒç»„ä»¶å¯¼å…¥
from conf_analysis.core.analyzer import UnifiedAnalyzer
from conf_analysis.core.scrapers.base_scraper import BaseScraper
from conf_analysis.core.scrapers.aaai_scraper import AAAIScraper
from conf_analysis.core.scrapers.iclr_scraper import ICLRScraper
from conf_analysis.core.scrapers.icml_scraper import ICMLScraper
from conf_analysis.core.scrapers.ijcai_scraper import IJCAIScraper
from conf_analysis.core.scrapers.neurips_scraper import NeuRIPSScraper
from conf_analysis.core.utils.pdf_manager import PDFDownloader, PDFManager
from conf_analysis.core.embeddings.text_encoder import PaperTextEncoder
from conf_analysis.core.database.milvus_client import MilvusClient, MilvusClientConfig
from conf_analysis.core.database.simple_vector_store import SimpleVectorStore
from conf_analysis.core.models.paper import Paper
from conf_analysis.core.utils.config import CONFERENCES

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class IntegratedPaperAnalysisSystem:
    """é›†æˆè®ºæ–‡åˆ†æç³»ç»Ÿ - å®Œæ•´çš„ç«¯åˆ°ç«¯å·¥ä½œæµ"""
    
    def __init__(self, 
                 conferences: Optional[List[str]] = None,
                 years: Optional[List[int]] = None,
                 enable_milvus: bool = True,
                 enable_pdf_download: bool = True):
        """
        åˆå§‹åŒ–é›†æˆåˆ†æç³»ç»Ÿ
        
        Args:
            conferences: è¦å¤„ç†çš„ä¼šè®®åˆ—è¡¨ ['AAAI', 'ICLR', 'ICML', 'IJCAI', 'NeuRIPS']
            years: è¦å¤„ç†çš„å¹´ä»½åˆ—è¡¨ [2018, 2019, ..., 2024]
            enable_milvus: æ˜¯å¦å¯ç”¨Milvuså‘é‡æ•°æ®åº“
            enable_pdf_download: æ˜¯å¦å¯ç”¨PDFä¸‹è½½
        """
        # ä½¿ç”¨config.pyä¸­çš„ä¼šè®®å’Œå¹´ä»½é…ç½®
        if conferences is None:
            self.conferences = list(CONFERENCES.keys())
        else:
            self.conferences = conferences
        
        if years is None:
            # ä»æ‰€æœ‰ä¼šè®®ä¸­è·å–å¹´ä»½èŒƒå›´çš„å¹¶é›†
            all_years = set()
            for conf_config in CONFERENCES.values():
                all_years.update(conf_config.get('years', []))
            self.years = sorted(list(all_years))
        else:
            self.years = years
        self.enable_milvus = enable_milvus
        self.enable_pdf_download = enable_pdf_download
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.scrapers = self._initialize_scrapers()
        self.analyzer = UnifiedAnalyzer()
        self.pdf_manager = PDFManager() if enable_pdf_download else None
        self.text_encoder = PaperTextEncoder() if enable_milvus else None
        self.milvus_client = None
        self.simple_vector_store = None
        
        # çŠ¶æ€è·Ÿè¸ª
        self.progress = {
            'scraping': {'completed': 0, 'total': 0, 'status': 'pending'},
            'pdf_download': {'completed': 0, 'total': 0, 'status': 'pending'},
            'vector_encoding': {'completed': 0, 'total': 0, 'status': 'pending'},
            'milvus_storage': {'completed': 0, 'total': 0, 'status': 'pending'},
            'analysis': {'status': 'pending'}
        }
        
        logger.info(f"Initialized system for conferences: {self.conferences}, years: {self.years}")
    
    def _initialize_scrapers(self) -> Dict[str, BaseScraper]:
        """åˆå§‹åŒ–çˆ¬è™«"""
        scrapers = {}
        
        scraper_classes = {
            'AAAI': AAAIScraper,
            'ICLR': ICLRScraper,
            'ICML': ICMLScraper,
            'IJCAI': IJCAIScraper,
            'NeuRIPS': NeuRIPSScraper
        }
        
        for conf in self.conferences:
            if conf in scraper_classes:
                try:
                    scrapers[conf] = scraper_classes[conf]()
                    logger.info(f"Initialized {conf} scraper")
                except Exception as e:
                    logger.error(f"Failed to initialize {conf} scraper: {e}")
            else:
                logger.warning(f"Unknown conference: {conf}")
        
        return scrapers
    
    def _initialize_milvus(self) -> bool:
        """åˆå§‹åŒ–Milvuså®¢æˆ·ç«¯ï¼Œå¤±è´¥æ—¶å›é€€åˆ°æœ¬åœ°å‘é‡å­˜å‚¨"""
        if not self.enable_milvus:
            return False
        
        # è·å–å‘é‡ç»´åº¦
        vector_dim = self.text_encoder.get_embedding_dim() if self.text_encoder else 768
        
        # é¦–å…ˆå°è¯•Milvus
        try:
            config = MilvusClientConfig.from_env()
            self.milvus_client = MilvusClient(config, vector_dim)
            logger.info(f"âœ… Initialized Milvus client with vector dimension: {vector_dim}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Failed to initialize Milvus client: {e}")
            logger.info("ğŸ”„ Falling back to local vector store...")
            
            # å›é€€åˆ°ç®€å•å‘é‡å­˜å‚¨
            try:
                self.simple_vector_store = SimpleVectorStore(vector_dim=vector_dim)
                if self.simple_vector_store.connect():
                    logger.info(f"âœ… Initialized simple vector store with vector dimension: {vector_dim}")
                    return True
                else:
                    logger.error("âŒ Failed to initialize simple vector store")
                    self.enable_milvus = False
                    return False
                    
            except Exception as e2:
                logger.error(f"âŒ Failed to initialize simple vector store: {e2}")
                logger.warning("âš ï¸ Continuing without vector storage...")
                self.enable_milvus = False
                return False
    
    async def scrape_papers(self) -> Dict[str, int]:
        """çˆ¬å–è®ºæ–‡å…ƒæ•°æ®"""
        print("\nğŸ•·ï¸ å¼€å§‹çˆ¬å–è®ºæ–‡å…ƒæ•°æ®...")
        self.progress['scraping']['status'] = 'running'
        
        total_papers = 0
        results = {}
        
        # è®¡ç®—æ€»ä»»åŠ¡æ•°
        self.progress['scraping']['total'] = len(self.conferences) * len(self.years)
        
        for conf in self.conferences:
            if conf not in self.scrapers:
                logger.warning(f"Scraper for {conf} not available")
                continue
            
            scraper = self.scrapers[conf]
            conf_results = {}
            
            for year in self.years:
                try:
                    print(f"  ğŸ“‹ çˆ¬å– {conf} {year}...")
                    
                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨æ•°æ®æ–‡ä»¶
                    json_file = Path(f"outputs/data/raw/{conf}_{year}.json")
                    if json_file.exists():
                        with open(json_file, 'r', encoding='utf-8') as f:
                            papers = json.load(f)
                        print(f"    âœ… ä½¿ç”¨å·²å­˜åœ¨æ•°æ®: {len(papers)} ç¯‡è®ºæ–‡")
                    else:
                        # çˆ¬å–æ–°æ•°æ®
                        papers = scraper.get_papers_for_year(year)
                        
                        # ä¿å­˜æ•°æ®
                        json_file.parent.mkdir(parents=True, exist_ok=True)
                        with open(json_file, 'w', encoding='utf-8') as f:
                            json.dump(papers, f, ensure_ascii=False, indent=2)
                        
                        print(f"    âœ… çˆ¬å–å®Œæˆ: {len(papers)} ç¯‡è®ºæ–‡")
                    
                    conf_results[year] = len(papers)
                    total_papers += len(papers)
                    
                    # æ›´æ–°è¿›åº¦
                    self.progress['scraping']['completed'] += 1
                    self._print_progress('scraping')
                    
                except Exception as e:
                    logger.error(f"Failed to scrape {conf} {year}: {e}")
                    conf_results[year] = 0
                    self.progress['scraping']['completed'] += 1
            
            results[conf] = conf_results
        
        self.progress['scraping']['status'] = 'completed'
        print(f"\nâœ… è®ºæ–‡çˆ¬å–å®Œæˆï¼æ€»è®¡: {total_papers:,} ç¯‡è®ºæ–‡")
        
        return results
    
    async def download_pdfs(self) -> Dict[str, int]:
        """ä¸‹è½½PDFæ–‡ä»¶"""
        if not self.enable_pdf_download:
            print("\nâ­ï¸ è·³è¿‡PDFä¸‹è½½")
            return {'downloaded': 0, 'failed': 0, 'skipped': 0}
        
        print("\nğŸ“¥ å¼€å§‹ä¸‹è½½PDFæ–‡ä»¶...")
        self.progress['pdf_download']['status'] = 'running'
        
        try:
            # æ£€æŸ¥ç°æœ‰PDFçŠ¶æ€
            status = self.pdf_manager.get_download_status()
            print(f"ğŸ“Š å½“å‰PDFçŠ¶æ€: {status['total_pdfs']:,}/{status['total_papers']:,} ({status['download_rate']:.1%})")
            
            # æ‰§è¡Œå¢é‡ä¸‹è½½
            async with PDFDownloader() as downloader:
                stats = await downloader.download_all_papers(
                    conferences=self.conferences,
                    years=self.years
                )
            
            self.progress['pdf_download']['status'] = 'completed'
            print(f"\nâœ… PDFä¸‹è½½å®Œæˆï¼")
            print(f"   ğŸ“¥ æ–°ä¸‹è½½: {stats['downloaded']}")
            print(f"   âŒ å¤±è´¥: {stats['failed']}")
            print(f"   â­ï¸ è·³è¿‡: {stats['skipped']}")
            
            return stats
            
        except Exception as e:
            logger.error(f"PDF download failed: {e}")
            self.progress['pdf_download']['status'] = 'failed'
            return {'downloaded': 0, 'failed': 1, 'skipped': 0}
    
    def check_pdf_duplicates(self, papers_data: List[Dict]) -> Tuple[List[Dict], List[Dict]]:
        """æ£€æŸ¥PDFé‡å¤å’Œç¼ºå¤±æƒ…å†µ"""
        print("\nğŸ” æ£€æŸ¥PDFæ–‡ä»¶çŠ¶æ€...")
        
        existing_pdfs = []
        missing_pdfs = []
        
        pdf_base_dir = Path("outputs/data/pdfs")
        
        for paper in papers_data:
            title = paper.get('title', 'Unknown')
            conference = paper.get('conference', 'Unknown')
            year = paper.get('year', 2024)
            
            # ç”Ÿæˆå¯èƒ½çš„æ–‡ä»¶å
            if hasattr(self.pdf_manager, 'generate_filename'):
                filename = self.pdf_manager.generate_filename(title, conference, year)
            else:
                # ç®€å•çš„æ–‡ä»¶åç”Ÿæˆ
                safe_title = ''.join(c for c in title if c.isalnum() or c in (' ', '-', '_')).rstrip()
                safe_title = safe_title.replace(' ', '_')[:100]
                filename = f"{safe_title}_{year}.pdf"
            
            pdf_path = pdf_base_dir / conference / filename
            
            if pdf_path.exists() and pdf_path.stat().st_size > 1000:  # è‡³å°‘1KB
                existing_pdfs.append(paper)
            else:
                missing_pdfs.append(paper)
        
        print(f"   âœ… å·²å­˜åœ¨PDF: {len(existing_pdfs)}")
        print(f"   âŒ ç¼ºå¤±PDF: {len(missing_pdfs)}")
        
        return existing_pdfs, missing_pdfs
    
    def filter_unencoded_papers(self, papers_data: List[Dict]) -> Tuple[List[Dict], int, int]:
        """è¿‡æ»¤å‡ºæœªç¼–ç çš„è®ºæ–‡"""
        if not self.enable_milvus:
            return papers_data, len(papers_data), 0
        
        print(f"\nğŸ” æ£€æŸ¥å·²ç¼–ç è®ºæ–‡çŠ¶æ€...")
        
        # ç”Ÿæˆè®ºæ–‡ID
        paper_ids = []
        for i, paper_data in enumerate(papers_data):
            paper_id = paper_data.get('id') or f"{paper_data.get('conference', 'UNK')}_{paper_data.get('year', 2024)}_{i}"
            paper_ids.append(paper_id)
            paper_data['generated_id'] = paper_id  # ä¿å­˜ç”Ÿæˆçš„ID
        
        # æ£€æŸ¥å·²å­˜åœ¨çš„è®ºæ–‡
        if not self.milvus_client and not self.simple_vector_store and self.enable_milvus:
            # å»¶è¿Ÿåˆå§‹åŒ–å‘é‡å­˜å‚¨
            self._initialize_milvus()
        
        storage_client = self.milvus_client or self.simple_vector_store
        if storage_client:
            try:
                existing_ids = storage_client.get_existing_paper_ids(
                    conferences=self.conferences,
                    years=self.years
                )
            except Exception as e:
                logger.warning(f"Failed to check existing papers: {e}")
                existing_ids = set()
        else:
            existing_ids = set()
        
        # è¿‡æ»¤æœªç¼–ç çš„è®ºæ–‡
        unencoded_papers = []
        for paper_data in papers_data:
            paper_id = paper_data['generated_id']
            if paper_id not in existing_ids:
                unencoded_papers.append(paper_data)
        
        existing_count = len(papers_data) - len(unencoded_papers)
        print(f"   ğŸ“Š æ€»è®ºæ–‡: {len(papers_data)}")
        print(f"   âœ… å·²ç¼–ç : {existing_count}")
        print(f"   ğŸ†• å¾…ç¼–ç : {len(unencoded_papers)}")
        
        return unencoded_papers, len(unencoded_papers), existing_count

    def encode_papers_to_vectors(self, papers_data: List[Dict]) -> Tuple[List[Paper], int]:
        """å°†è®ºæ–‡ç¼–ç ä¸ºå‘é‡ï¼ˆåªå¤„ç†æœªç¼–ç çš„è®ºæ–‡ï¼‰"""
        if not self.enable_milvus or not self.text_encoder:
            print("\nâ­ï¸ è·³è¿‡å‘é‡ç¼–ç ")
            return [], 0
        
        # è¿‡æ»¤æœªç¼–ç çš„è®ºæ–‡
        unencoded_papers, unencoded_count, existing_count = self.filter_unencoded_papers(papers_data)
        
        if unencoded_count == 0:
            print("\nâœ… æ‰€æœ‰è®ºæ–‡å·²ç¼–ç ï¼Œè·³è¿‡å‘é‡ç¼–ç æ­¥éª¤")
            return [], 0
        
        print(f"\nğŸ§® å¼€å§‹å‘é‡ç¼–ç  {unencoded_count} ç¯‡æ–°è®ºæ–‡...")
        self.progress['vector_encoding']['status'] = 'running'
        self.progress['vector_encoding']['total'] = unencoded_count
        
        paper_objects = []
        success_count = 0
        
        # æ‰¹é‡ç¼–ç æ–‡æœ¬
        try:
            print("   ğŸ“ æ‰¹é‡ç¼–ç æ–‡æœ¬å†…å®¹...")
            text_embeddings, semantic_embeddings = self.text_encoder.batch_encode_papers(unencoded_papers)
            
            print(f"   âœ… ç¼–ç å®Œæˆ: {text_embeddings.shape[0]} ç¯‡è®ºæ–‡")
            
            # åˆ›å»ºPaperå¯¹è±¡
            for i, paper_data in enumerate(unencoded_papers):
                try:
                    # åˆ›å»ºPaperå¯¹è±¡ï¼ˆæ ¹æ®Paperç±»çš„æ„é€ å‡½æ•°ç­¾åï¼‰
                    paper = Paper(
                        title=paper_data.get('title', ''),
                        abstract=paper_data.get('abstract', ''),
                        conference=paper_data.get('conference', ''),
                        year=int(paper_data.get('year', 2024)),
                        url=paper_data.get('url', ''),
                        pdf_url=paper_data.get('pdf_url', ''),
                        paper_id=paper_data.get('generated_id', f"{paper_data.get('conference', 'UNK')}_{paper_data.get('year', 2024)}_{i}")
                    )
                    
                    # æ·»åŠ å…¶ä»–ä¿¡æ¯
                    if paper_data.get('keywords'):
                        paper.keywords = paper_data['keywords']
                    if paper_data.get('authors'):
                        from conf_analysis.core.models.paper import AuthorInfo
                        author_info = AuthorInfo(names=paper_data['authors'])
                        paper.add_author_info(author_info)
                    
                    # è®¾ç½®å‘é‡
                    if i < len(text_embeddings):
                        paper.set_text_vector(text_embeddings[i])
                    if i < len(semantic_embeddings):
                        paper.set_semantic_vector(semantic_embeddings[i])
                    
                    # åˆ†æä»»åŠ¡åœºæ™¯
                    paper.analyze_task_scenario()
                    
                    paper_objects.append(paper)
                    success_count += 1
                    
                    self.progress['vector_encoding']['completed'] += 1
                    
                    if success_count % 100 == 0:
                        self._print_progress('vector_encoding')
                        
                except Exception as e:
                    logger.error(f"Failed to process paper {i}: {e}")
            
            self.progress['vector_encoding']['status'] = 'completed'
            print(f"\nâœ… å‘é‡ç¼–ç å®Œæˆï¼æ–°ç¼–ç : {success_count}/{unencoded_count}, æ€»è®¡è·³è¿‡: {existing_count}")
            
        except Exception as e:
            logger.error(f"Batch encoding failed: {e}")
            self.progress['vector_encoding']['status'] = 'failed'
        
        return paper_objects, success_count
    
    def store_to_milvus(self, paper_objects: List[Paper]) -> int:
        """å­˜å‚¨åˆ°Milvusæ•°æ®åº“"""
        if not self.enable_milvus or not paper_objects:
            print("\nâ­ï¸ è·³è¿‡Milvuså­˜å‚¨")
            return 0
        
        # åˆå§‹åŒ–å‘é‡å­˜å‚¨è¿æ¥
        if not self.milvus_client and not self.simple_vector_store:
            if not self._initialize_milvus():
                return 0
        
        # ç¡®å®šä½¿ç”¨å“ªç§å­˜å‚¨
        storage_type = "Milvus" if self.milvus_client else "Simple Vector Store"
        storage_client = self.milvus_client or self.simple_vector_store
        
        print(f"\nğŸ—„ï¸ å¼€å§‹å­˜å‚¨åˆ°{storage_type}: {len(paper_objects)} ç¯‡è®ºæ–‡...")
        self.progress['milvus_storage']['status'] = 'running'
        self.progress['milvus_storage']['total'] = len(paper_objects)
        
        try:
            # æ‰¹é‡æ’å…¥
            success_count, total_count = storage_client.batch_insert_papers(
                paper_objects, batch_size=100
            )
            
            self.progress['milvus_storage']['completed'] = success_count
            self.progress['milvus_storage']['status'] = 'completed'
            
            print(f"\nâœ… {storage_type}å­˜å‚¨å®Œæˆï¼æˆåŠŸå­˜å‚¨: {success_count}/{total_count}")
            
            # æ˜¾ç¤ºé›†åˆä¿¡æ¯
            collection_info = storage_client.get_collection_info()
            if 'statistics' in collection_info:
                print(f"ğŸ“Š æ•°æ®åº“ç»Ÿè®¡: {collection_info['statistics']}")
            elif 'total_papers' in collection_info:
                print(f"ğŸ“Š å­˜å‚¨ç»Ÿè®¡: æ€»è®ºæ–‡æ•° {collection_info['total_papers']}")
            
            return success_count
            
        except Exception as e:
            logger.error(f"{storage_type} storage failed: {e}")
            self.progress['milvus_storage']['status'] = 'failed'
            return 0
    
    def perform_analysis(self) -> Optional[Dict]:
        """æ‰§è¡Œè®ºæ–‡åˆ†æ"""
        print("\nğŸ“Š å¼€å§‹åˆ†æè®ºæ–‡æ•°æ®...")
        self.progress['analysis']['status'] = 'running'
        
        try:
            # æ‰§è¡Œç»¼åˆåˆ†æ
            results = self.analyzer.perform_comprehensive_analysis()
            
            self.progress['analysis']['status'] = 'completed'
            print("\nâœ… åˆ†æå®Œæˆï¼")
            print(f"ğŸ“Š æ€»è®ºæ–‡æ•°: {results['basic_statistics']['total_papers']:,}")
            print(f"ğŸ“… æ—¶é—´è·¨åº¦: {results['basic_statistics']['year_range']}")
            print(f"ğŸ›ï¸ è¦†ç›–ä¼šè®®: {len(results['basic_statistics']['conferences'])}ä¸ª")
            print(f"ğŸ“ˆ æ•´ä½“å¢é•¿ç‡: {results['temporal_analysis']['total_growth_rate']:.1f}%")
            
            return results
            
        except Exception as e:
            logger.error(f"Analysis failed: {e}")
            self.progress['analysis']['status'] = 'failed'
            return None
    
    def generate_reports(self, analysis_results: Dict) -> bool:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        print("\nğŸ“‹ ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        
        try:
            # è¯»å–æ¨¡æ¿
            template_file = Path("frontend/unified_analysis_dashboard.html")
            if not template_file.exists():
                logger.error(f"Dashboard template not found at {template_file}")
                print(f"âŒ æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨: {template_file}")
                # åˆ›å»ºç®€å•çš„HTMLæŠ¥å‘Š
                self._create_simple_report(analysis_results)
                return True
            
            with open(template_file, 'r', encoding='utf-8') as f:
                html_content = f.read()
            
            # åµŒå…¥æ•°æ®
            data_script = f'''<script>
                window.embeddedAnalysisData = {json.dumps(analysis_results, ensure_ascii=False, indent=4)};
            </script>'''
            
            html_content = html_content.replace('</head>', f'{data_script}\n</head>')
            
            # ä¿å­˜æŠ¥å‘Š
            output_file = Path("outputs/unified_analysis_report.html")
            output_file.parent.mkdir(parents=True, exist_ok=True)
            
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(html_content)
            
            print(f"ğŸ“Š ç»Ÿä¸€åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")
            print(f"ğŸŒ ä½¿ç”¨æµè§ˆå™¨æ‰“å¼€æŸ¥çœ‹: {output_file.absolute()}")
            
            # æ˜¾ç¤ºå…³é”®ç»“æœ
            self._display_key_results(analysis_results)
            
            return True
            
        except Exception as e:
            logger.error(f"Report generation failed: {e}")
            return False
    
    def _create_simple_report(self, analysis_results: Dict) -> None:
        """åˆ›å»ºç®€å•çš„HTMLæŠ¥å‘Š"""
        try:
            # åˆ›å»ºJSONå®‰å…¨çš„æ•°æ®å‰¯æœ¬
            def make_json_safe(obj):
                if isinstance(obj, dict):
                    return {str(k): make_json_safe(v) for k, v in obj.items() if not isinstance(k, tuple)}
                elif isinstance(obj, list):
                    return [make_json_safe(item) for item in obj]
                elif isinstance(obj, tuple):
                    return list(obj)
                elif hasattr(obj, '__dict__'):
                    return str(obj)
                else:
                    return obj
            
            safe_results = make_json_safe(analysis_results)
            
            html_content = f"""<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AIä¼šè®®è®ºæ–‡åˆ†ææŠ¥å‘Š</title>
    <style>
        body {{ font-family: 'Microsoft YaHei', Arial, sans-serif; margin: 40px; }}
        .header {{ text-align: center; margin-bottom: 40px; }}
        .stat-card {{ background: #f8f9fa; padding: 20px; margin: 20px 0; border-radius: 8px; }}
        .section {{ margin: 30px 0; }}
        .list-item {{ margin: 10px 0; padding: 10px; background: #fff; border-left: 4px solid #007bff; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸš€ AIä¼šè®®è®ºæ–‡åˆ†ææŠ¥å‘Š</h1>
        <p>ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
    </div>
    
    <div class="stat-card">
        <h2>ğŸ“Š åŸºç¡€ç»Ÿè®¡</h2>
        <p><strong>æ€»è®ºæ–‡æ•°:</strong> {analysis_results.get('basic_statistics', {}).get('total_papers', 0):,}</p>
        <p><strong>æ—¶é—´è·¨åº¦:</strong> {analysis_results.get('basic_statistics', {}).get('year_range', 'N/A')}</p>
        <p><strong>ä¼šè®®æ•°é‡:</strong> {len(analysis_results.get('basic_statistics', {}).get('conferences', []))}</p>
        <p><strong>å¢é•¿ç‡:</strong> {analysis_results.get('temporal_analysis', {}).get('total_growth_rate', 0):.1f}%</p>
    </div>
    
    <div class="section">
        <h2>ğŸ¯ çƒ­é—¨åº”ç”¨åœºæ™¯</h2>"""
            
            # æ·»åŠ åº”ç”¨åœºæ™¯æ•°æ®
            if 'task_scenario_analysis' in analysis_results:
                scenarios = analysis_results['task_scenario_analysis'].get('top_scenarios', [])
                for i, scenario in enumerate(scenarios[:10], 1):
                    count = analysis_results['task_scenario_analysis']['scenario_distribution'].get(scenario, 0)
                    html_content += f"""
        <div class="list-item">
            <strong>{i}. {scenario}</strong> - {count} ç¯‡è®ºæ–‡
        </div>"""
            
            html_content += """
    </div>
    
    <div class="section">
        <h2>ğŸ”¥ æ–°å…´æŠ€æœ¯è¶‹åŠ¿</h2>"""
            
            # æ·»åŠ æ–°å…´è¶‹åŠ¿æ•°æ®
            if 'emerging_trends' in analysis_results:
                emerging = analysis_results['emerging_trends'].get('emerging_application_scenarios', {})
                for scenario, data in list(emerging.items())[:5]:
                    html_content += f"""
        <div class="list-item">
            <strong>{scenario}</strong> - å¢é•¿ç‡: +{data.get('growth_rate', 0)}%
        </div>"""
            
            html_content += """
    </div>
    
    <div class="section">
        <h2>ğŸ“ˆ æ•°æ®å¯è§†åŒ–</h2>
        <p>å®Œæ•´çš„äº¤äº’å¼æ•°æ®å¯è§†åŒ–éœ€è¦è¿è¡Œå®Œæ•´ç‰ˆæœ¬çš„ä»ªè¡¨æ¿ã€‚</p>
        <p>è¯¦ç»†æ•°æ®è¯·æŸ¥çœ‹: <code>outputs/analysis/comprehensive_analysis.json</code></p>
    </div>
    
    <script>
        // æ•°æ®æ‘˜è¦ä¿¡æ¯
        window.analysisSummary = {
            total_papers: """ + str(analysis_results.get('basic_statistics', {}).get('total_papers', 0)) + """,
            conferences: """ + str(len(analysis_results.get('basic_statistics', {}).get('conferences', []))) + """,
            year_range: '""" + str(analysis_results.get('basic_statistics', {}).get('year_range', 'N/A')) + """',
            growth_rate: """ + str(analysis_results.get('temporal_analysis', {}).get('total_growth_rate', 0)) + """
        };
    </script>
</body>
</html>"""
            
            # ä¿å­˜ç®€å•æŠ¥å‘Š
            output_file = Path("outputs/unified_analysis_report.html")
            output_file.parent.mkdir(parents=True, exist_ok=True)
            
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(html_content)
            
            print(f"ğŸ“Š ç®€åŒ–åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")
            
        except Exception as e:
            logger.error(f"Failed to create simple report: {e}")
            import traceback
            traceback.print_exc()
    
    def _display_key_results(self, results: Dict) -> None:
        """æ˜¾ç¤ºå…³é”®åˆ†æç»“æœ"""
        # æ˜¾ç¤ºçƒ­é—¨åº”ç”¨åœºæ™¯
        if 'task_scenario_analysis' in results and results['task_scenario_analysis']:
            scenarios = results['task_scenario_analysis'].get('top_scenarios', [])
            if scenarios:
                print(f"\nğŸ¯ çƒ­é—¨åº”ç”¨åœºæ™¯:")
                for i, scenario in enumerate(scenarios[:5], 1):
                    count = results['task_scenario_analysis']['scenario_distribution'].get(scenario, 0)
                    print(f"   {i}. {scenario} ({count} ç¯‡)")
        
        # æ˜¾ç¤ºæ–°å…´è¶‹åŠ¿
        if 'emerging_trends' in results:
            emerging = results['emerging_trends'].get('emerging_application_scenarios', {})
            if emerging:
                print(f"\nğŸ”¥ æ–°å…´æŠ€æœ¯è¶‹åŠ¿:")
                for scenario, data in list(emerging.items())[:3]:
                    print(f"   ğŸ“ˆ {scenario}: +{data['growth_rate']}% å¢é•¿")
    
    def _print_progress(self, task: str) -> None:
        """æ‰“å°è¿›åº¦ä¿¡æ¯"""
        progress = self.progress[task]
        if progress['total'] > 0:
            percentage = (progress['completed'] / progress['total']) * 100
            print(f"    è¿›åº¦: {progress['completed']}/{progress['total']} ({percentage:.1f}%)")
    
    def print_final_summary(self) -> None:
        """æ‰“å°æœ€ç»ˆæ‘˜è¦"""
        print("\n" + "=" * 80)
        print("ğŸ‰ AIä¼šè®®è®ºæ–‡åˆ†æç³»ç»Ÿ - æ‰§è¡Œå®Œæˆ")
        print("=" * 80)
        
        for task, progress in self.progress.items():
            status_emoji = {
                'completed': 'âœ…',
                'running': 'ğŸ”„',
                'failed': 'âŒ',
                'pending': 'â¸ï¸'
            }.get(progress['status'], 'â“')
            
            task_names = {
                'scraping': 'è®ºæ–‡çˆ¬å–',
                'pdf_download': 'PDFä¸‹è½½',
                'vector_encoding': 'å‘é‡ç¼–ç ',
                'milvus_storage': 'Milvuså­˜å‚¨',
                'analysis': 'æ•°æ®åˆ†æ'
            }
            
            task_name = task_names.get(task, task)
            print(f"{status_emoji} {task_name}: {progress['status']}")
            
            if 'total' in progress and progress['total'] > 0:
                print(f"   å®Œæˆ: {progress['completed']}/{progress['total']}")
        
        print("\nğŸ“‚ è¾“å‡ºç›®å½•ç»“æ„:")
        print("   outputs/")
        print("   â”œâ”€â”€ data/raw/              # è®ºæ–‡å…ƒæ•°æ®JSON")
        print("   â”œâ”€â”€ data/pdfs/             # PDFæ–‡ä»¶")
        print("   â”œâ”€â”€ analysis/              # åˆ†æç»“æœæ•°æ®")
        print("   â””â”€â”€ unified_analysis_report.html # åˆ†ææŠ¥å‘Š")
        
        if self.enable_milvus:
            print("\nğŸ—„ï¸ Milvusæ•°æ®åº“å·²å°±ç»ªï¼Œæ”¯æŒä»¥ä¸‹åŠŸèƒ½:")
            print("   â€¢ è¯­ä¹‰ç›¸ä¼¼è®ºæ–‡æœç´¢")
            print("   â€¢ ä»»åŠ¡åœºæ™¯åˆ†ç±»æ£€ç´¢")
            print("   â€¢ æ··åˆæŸ¥è¯¢ (æ–‡æœ¬+è¯­ä¹‰)")
            print("   â€¢ æ™ºèƒ½å¢é‡ç¼–ç  (åªå¤„ç†æ–°è®ºæ–‡)")
        
        print("\n" + "=" * 80)
    
    async def run_complete_pipeline(self) -> Dict:
        """è¿è¡Œå®Œæ•´çš„ç«¯åˆ°ç«¯æµæ°´çº¿"""
        print("ğŸš€ å¯åŠ¨ AIä¼šè®®è®ºæ–‡åˆ†æç³»ç»Ÿ - å®Œæ•´æµæ°´çº¿")
        print("=" * 60)
        print(f"ğŸ“‹ ä¼šè®®: {', '.join(self.conferences)}")
        print(f"ğŸ“… å¹´ä»½: {self.years[0]}-{self.years[-1]}")
        print(f"ğŸ—„ï¸ Milvuså­˜å‚¨: {'å¯ç”¨' if self.enable_milvus else 'ç¦ç”¨'}")
        print(f"ğŸ“¥ PDFä¸‹è½½: {'å¯ç”¨' if self.enable_pdf_download else 'ç¦ç”¨'}")
        print("=" * 60)
        
        pipeline_results = {}
        
        try:
            # 1. çˆ¬å–è®ºæ–‡å…ƒæ•°æ®
            scraping_results = await self.scrape_papers()
            pipeline_results['scraping'] = scraping_results
            
            # 2. åŠ è½½æ‰€æœ‰è®ºæ–‡æ•°æ®
            print("\nğŸ“š åŠ è½½è®ºæ–‡æ•°æ®...")
            all_papers_data = []
            
            for conf in self.conferences:
                for year in self.years:
                    json_file = Path(f"outputs/data/raw/{conf}_{year}.json")
                    if json_file.exists():
                        with open(json_file, 'r', encoding='utf-8') as f:
                            papers = json.load(f)
                            # æ·»åŠ conferenceå’Œyearä¿¡æ¯
                            for paper in papers:
                                paper['conference'] = conf
                                paper['year'] = year
                            all_papers_data.extend(papers)
            
            print(f"   ğŸ“Š æ€»è®¡åŠ è½½: {len(all_papers_data)} ç¯‡è®ºæ–‡")
            
            # 3. æ£€æŸ¥PDFçŠ¶æ€
            if self.enable_pdf_download:
                existing_pdfs, missing_pdfs = self.check_pdf_duplicates(all_papers_data)
                pipeline_results['pdf_status'] = {
                    'existing': len(existing_pdfs),
                    'missing': len(missing_pdfs)
                }
            
            # 4. ä¸‹è½½ç¼ºå¤±çš„PDF
            pdf_results = await self.download_pdfs()
            pipeline_results['pdf_download'] = pdf_results
            
            # 5. å‘é‡ç¼–ç ï¼ˆæ™ºèƒ½è·³è¿‡å·²ç¼–ç è®ºæ–‡ï¼‰
            paper_objects, encoding_count = self.encode_papers_to_vectors(all_papers_data)
            pipeline_results['vector_encoding'] = encoding_count
            
            # 6. å­˜å‚¨åˆ°Milvus
            milvus_count = self.store_to_milvus(paper_objects)
            pipeline_results['milvus_storage'] = milvus_count
            
            # 7. æ‰§è¡Œåˆ†æ
            analysis_results = self.perform_analysis()
            if analysis_results:
                pipeline_results['analysis'] = 'success'
                
                # 8. ç”ŸæˆæŠ¥å‘Š
                report_success = self.generate_reports(analysis_results)
                pipeline_results['reports'] = 'success' if report_success else 'failed'
            else:
                pipeline_results['analysis'] = 'failed'
                pipeline_results['reports'] = 'skipped'
            
            return pipeline_results
            
        except Exception as e:
            logger.error(f"Pipeline execution failed: {e}")
            pipeline_results['error'] = str(e)
            return pipeline_results
        
        finally:
            # æ¸…ç†èµ„æº
            if self.milvus_client:
                self.milvus_client.disconnect()
            
            # æ‰“å°æœ€ç»ˆæ‘˜è¦
            self.print_final_summary()


def main(conferences: Optional[List[str]] = None, 
         years: Optional[List[int]] = None,
         enable_milvus: bool = True,
         enable_pdf_download: bool = True):
    """ä¸»å‡½æ•° - å¯åŠ¨å®Œæ•´çš„è®ºæ–‡åˆ†ææµæ°´çº¿"""
    
    # åˆ›å»ºé›†æˆç³»ç»Ÿ
    system = IntegratedPaperAnalysisSystem(
        conferences=conferences,
        years=years,
        enable_milvus=enable_milvus,
        enable_pdf_download=enable_pdf_download
    )
    
    # è¿è¡Œå®Œæ•´æµæ°´çº¿
    try:
        results = asyncio.run(system.run_complete_pipeline())
        return results
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        return None
    except Exception as e:
        logger.error(f"Main execution failed: {e}")
        return None


if __name__ == "__main__":
    main()